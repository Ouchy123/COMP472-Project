{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Dataset Setup"
      ],
      "metadata": {
        "id": "E1Q4AmqJQjq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lkgosKBJWNj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "from joblib import dump, load\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89_h5D2PQq1"
      },
      "source": [
        "## Resizing images and normalizing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGI8TxBRPYCp"
      },
      "outputs": [],
      "source": [
        "transforms.resnet = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE16cKo-Q_GU"
      },
      "source": [
        "### Loading dataset (CIFAR-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k73ePpbLRHkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326be208-6899-48aa-c4e5-4871b3d308c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:14<00:00, 11.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.resnet)\n",
        "testset_full = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ua6umg6SWkx"
      },
      "source": [
        "Selecting 500 training images and 100 test images per class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6r_VujwSfbA"
      },
      "outputs": [],
      "source": [
        "def get_subset(dataset, indices):\n",
        "    target = np.array(dataset.targets)\n",
        "    selected_indices = []\n",
        "    for i in range(10):\n",
        "        i_indices = np.where(target == i)[0][:indices]\n",
        "        selected_indices.extend(i_indices)\n",
        "    return Subset(dataset, selected_indices)\n",
        "\n",
        "trainset = get_subset(trainset_full, 500)\n",
        "testset = get_subset(testset_full, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1G3867bV2Qb"
      },
      "source": [
        "### Loading pretrained ResNet-18 and removing the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "CuNFG3mYWQCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f33062f6-cf17-43ac-d789-214d722eddb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 126MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
        "feature_extractor.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kMc7i6DXlw2"
      },
      "source": [
        "### Extract feature vector to get 512x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zlJjGfXKa60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d2f462-d32e-4c2f-f3b6-b17be63071e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 512) (1000, 512)\n"
          ]
        }
      ],
      "source": [
        "def extract_features(dataset, model, batch_size=64):\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, batch_labels in dataloader:\n",
        "            outputs = model(images)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs)\n",
        "            labels.append(batch_labels)\n",
        "    features = torch.cat(features).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(trainset, feature_extractor)\n",
        "test_features, test_labels = extract_features(testset, feature_extractor)\n",
        "\n",
        "print(train_features.shape, test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwRh5vdOd-vT"
      },
      "source": [
        "## Using PCA to reduce the size of feature vector from 512x1 to 50x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdUiyYSaeFl-",
        "outputId": "d12f05ba-e331-4751-ac15-0c8b2111789b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 50) (1000, 50)\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=50)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "test_features_pca = pca.transform(test_features)\n",
        "\n",
        "print(train_features_pca.shape, test_features_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to extract metrics"
      ],
      "metadata": {
        "id": "yutH1YxsheLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def metrics_row(y_true, y_pred, model_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='macro', zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Macro Precision\": prec,\n",
        "        \"Macro Recall\": rec,\n",
        "        \"Macro F1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "8T-oo3u8hkTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Naive Bayes"
      ],
      "metadata": {
        "id": "R1_896xfQtku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEbqudEgkSz_"
      },
      "source": [
        "Part3.1.1 - Build confusion matrix (rows=true labels, cols=predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p1SDVCVkJnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=10):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA2qpAbEkpsc"
      },
      "source": [
        "Part3.1.2 - Compute precision, recall, F1, and accuracy (macro + per-class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrSfHN6WklkU"
      },
      "outputs": [],
      "source": [
        "def precision_recall_f1_from_cm(cm, eps=1e-12):\n",
        "    tp = np.diag(cm).astype(np.float64)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall    = tp / (tp + fn + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "\n",
        "    return {\n",
        "        \"per_class_precision\": precision,\n",
        "        \"per_class_recall\": recall,\n",
        "        \"per_class_f1\": f1,\n",
        "        \"macro_precision\": precision.mean(),\n",
        "        \"macro_recall\": recall.mean(),\n",
        "        \"macro_f1\": f1.mean(),\n",
        "        \"accuracy\": tp.sum() / cm.sum()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQs9SP0slDqh"
      },
      "source": [
        "Part 3.1.3 — Pretty-print evaluation report with metrics and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1Jt_q3zk-ev"
      },
      "outputs": [],
      "source": [
        "def print_eval_report(name, y_true, y_pred, class_names=None):\n",
        "    cm = confusion_matrix_np(y_true, y_pred, num_classes=10)\n",
        "    m = precision_recall_f1_from_cm(cm)\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {m['accuracy']:.4f}\")\n",
        "    print(f\"- Macro Precision: {m['macro_precision']:.4f}\")\n",
        "    print(f\"- Macro Recall   : {m['macro_recall']:.4f}\")\n",
        "    print(f\"- Macro F1       : {m['macro_f1']:.4f}\")\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [str(i) for i in range(10)]\n",
        "    for i, cname in enumerate(class_names):\n",
        "        p = m[\"per_class_precision\"][i]\n",
        "        r = m[\"per_class_recall\"][i]\n",
        "        f = m[\"per_class_f1\"][i]\n",
        "        print(f\"  class {i} ({cname}): P={p:.4f} R={r:.4f} F1={f:.4f}\")\n",
        "\n",
        "cifar10_classes = [\n",
        "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
        "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztI1l3YLmJ3a"
      },
      "source": [
        "# Part 3.2.1 — Define Gaussian Naive Bayes class (NumPy only implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYYkJ6SHmHKu"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n",
        "    \"\"\"\n",
        "    NumPy-only Gaussian Naive Bayes:\n",
        "    - Estimate class priors, per-class means/variances\n",
        "    - Predict via log-likelihood + log-prior (argmax)\n",
        "    \"\"\"\n",
        "    def __init__(self, var_smoothing=1e-9):\n",
        "        self.var_smoothing = var_smoothing\n",
        "        self.classes_ = None\n",
        "        self.class_priors_ = None\n",
        "        self.means_ = None\n",
        "        self.vars_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        C = len(self.classes_)\n",
        "        N, D = X.shape\n",
        "\n",
        "        self.means_ = np.zeros((C, D), dtype=np.float64)\n",
        "        self.vars_  = np.zeros((C, D), dtype=np.float64)\n",
        "        self.class_priors_ = np.zeros(C, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            Xc = X[y == c]\n",
        "            self.means_[idx] = Xc.mean(axis=0)\n",
        "            self.vars_[idx]  = Xc.var(axis=0) + self.var_smoothing\n",
        "            self.class_priors_[idx] = len(Xc) / float(N)\n",
        "        return self\n",
        "\n",
        "    def _log_gaussian_likelihood(self, X):\n",
        "        X = np.asarray(X)\n",
        "        means = self.means_[None, :, :]   # (1, C, D)\n",
        "        vars_ = self.vars_[None, :, :]    # (1, C, D)\n",
        "        X_    = X[:, None, :]             # (N, 1, D)\n",
        "\n",
        "        log_term  = -0.5 * (np.log(2.0 * np.pi * vars_)).sum(axis=2)\n",
        "        quad_term = -0.5 * (((X_ - means) ** 2) / vars_).sum(axis=2)\n",
        "        return log_term + quad_term\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_like = self._log_gaussian_likelihood(X)\n",
        "        log_prior = np.log(self.class_priors_)[None, :]\n",
        "        scores = log_like + log_prior\n",
        "        idx = np.argmax(scores, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgGPBc48mSED"
      },
      "source": [
        "Part 3.2.2 — Fit method: estimate means, variances, and priors for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDrL6Ws0mNAM"
      },
      "outputs": [],
      "source": [
        "def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        C = len(self.classes_)\n",
        "        N, D = X.shape\n",
        "\n",
        "        self.means_ = np.zeros((C, D), dtype=np.float64)\n",
        "        self.vars_  = np.zeros((C, D), dtype=np.float64)\n",
        "        self.class_priors_ = np.zeros(C, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            Xc = X[y == c]\n",
        "            self.means_[idx] = Xc.mean(axis=0)\n",
        "            self.vars_[idx]  = Xc.var(axis=0) + self.var_smoothing\n",
        "            self.class_priors_[idx] = len(Xc) / float(N)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAWNb20mYde"
      },
      "source": [
        "Part 3.2.3 — Log-likelihood computation and prediction using argmax of scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mAJzXJCmZO1"
      },
      "outputs": [],
      "source": [
        "def _log_gaussian_likelihood(self, X):\n",
        "        X = np.asarray(X)\n",
        "        means = self.means_[None, :, :]   # (1, C, D)\n",
        "        vars_ = self.vars_[None, :, :]    # (1, C, D)\n",
        "        X_    = X[:, None, :]             # (N, 1, D)\n",
        "\n",
        "        log_term  = -0.5 * (np.log(2.0 * np.pi * vars_)).sum(axis=2)\n",
        "        quad_term = -0.5 * (((X_ - means) ** 2) / vars_).sum(axis=2)\n",
        "        return log_term + quad_term\n",
        "\n",
        "def predict(self, X):\n",
        "        log_like = self._log_gaussian_likelihood(X)\n",
        "        log_prior = np.log(self.class_priors_)[None, :]\n",
        "        scores = log_like + log_prior\n",
        "        idx = np.argmax(scores, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_M71BKCtF2T"
      },
      "source": [
        "Part 3.3.1 — Fit (train) the scratch GNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Ry4ECkstaK",
        "outputId": "b27c4417-4a9f-43bf-912d-384d16fec5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scratch GNB fitted on PCA-50 features.\n"
          ]
        }
      ],
      "source": [
        "gnb_scratch = GaussianNaiveBayes(var_smoothing=1e-9)\n",
        "gnb_scratch.fit(train_features_pca, train_labels)\n",
        "print(\"Scratch GNB fitted on PCA-50 features.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcF6dwZ-tOoX"
      },
      "source": [
        "Part 3.3.2 — Predict & print evaluation report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frybcqeLtLXD",
        "outputId": "09d07ad7-9b54-4cc4-8072-b498eb3678ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Gaussian Naive Bayes (Scratch, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[80  1  0  2  0  0  1  0 12  4]\n",
            " [ 3 89  0  2  1  0  0  0  0  5]\n",
            " [ 6  0 63  8  7  4 11  0  1  0]\n",
            " [ 1  0  3 74  4 10  7  1  0  0]\n",
            " [ 2  0  4  7 74  3  2  8  0  0]\n",
            " [ 0  1  6 13  3 74  2  1  0  0]\n",
            " [ 2  0  3  7  6  2 79  1  0  0]\n",
            " [ 2  1  0  4  8  5  0 79  1  0]\n",
            " [ 7  0  1  0  1  0  0  0 88  3]\n",
            " [ 5  3  0  1  0  0  0  1  1 89]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.7890\n",
            "- Macro Precision: 0.7937\n",
            "- Macro Recall   : 0.7890\n",
            "- Macro F1       : 0.7896\n",
            "  class 0 (airplane): P=0.7407 R=0.8000 F1=0.7692\n",
            "  class 1 (automobile): P=0.9368 R=0.8900 F1=0.9128\n",
            "  class 2 (bird): P=0.7875 R=0.6300 F1=0.7000\n",
            "  class 3 (cat): P=0.6271 R=0.7400 F1=0.6789\n",
            "  class 4 (deer): P=0.7115 R=0.7400 F1=0.7255\n",
            "  class 5 (dog): P=0.7551 R=0.7400 F1=0.7475\n",
            "  class 6 (frog): P=0.7745 R=0.7900 F1=0.7822\n",
            "  class 7 (horse): P=0.8681 R=0.7900 F1=0.8272\n",
            "  class 8 (ship): P=0.8544 R=0.8800 F1=0.8670\n",
            "  class 9 (truck): P=0.8812 R=0.8900 F1=0.8856\n"
          ]
        }
      ],
      "source": [
        "pred_scratch = gnb_scratch.predict(test_features_pca)\n",
        "print_eval_report(\n",
        "    \"Gaussian Naive Bayes (Scratch, PCA-50)\",\n",
        "    test_labels, pred_scratch,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "UuHSVnZDjrbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_gnb_scratch = metrics_row(test_labels, pred_scratch, \"Naive Bayes (Scratch)\")"
      ],
      "metadata": {
        "id": "Uiv_L0CujtwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPLkEWktPvB"
      },
      "source": [
        "## Part 3.4.1 — Gaussian Naive Bayes (scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6_h5ACOtP9o",
        "outputId": "57895990-244d-4abb-c203-38551b556980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn GNB fitted on PCA-50 features.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb_sklearn = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_sklearn.fit(train_features_pca, train_labels)\n",
        "print(\"sklearn GNB fitted on PCA-50 features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU0oesJqtq6f"
      },
      "source": [
        "Part 3.4.2 — Predict & print evaluation report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZwIyDdstrM8",
        "outputId": "b134c969-031d-4c8d-a7f0-94a36450b1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Gaussian Naive Bayes (scikit-learn, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[80  1  0  2  0  0  1  0 12  4]\n",
            " [ 3 89  0  2  1  0  0  0  0  5]\n",
            " [ 6  0 63  8  7  4 11  0  1  0]\n",
            " [ 1  0  3 74  4 10  7  1  0  0]\n",
            " [ 2  0  4  7 74  3  2  8  0  0]\n",
            " [ 0  1  6 13  3 74  2  1  0  0]\n",
            " [ 2  0  3  7  6  2 79  1  0  0]\n",
            " [ 2  1  0  4  8  5  0 79  1  0]\n",
            " [ 7  0  1  0  1  0  0  0 88  3]\n",
            " [ 5  3  0  1  0  0  0  1  1 89]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.7890\n",
            "- Macro Precision: 0.7937\n",
            "- Macro Recall   : 0.7890\n",
            "- Macro F1       : 0.7896\n",
            "  class 0 (airplane): P=0.7407 R=0.8000 F1=0.7692\n",
            "  class 1 (automobile): P=0.9368 R=0.8900 F1=0.9128\n",
            "  class 2 (bird): P=0.7875 R=0.6300 F1=0.7000\n",
            "  class 3 (cat): P=0.6271 R=0.7400 F1=0.6789\n",
            "  class 4 (deer): P=0.7115 R=0.7400 F1=0.7255\n",
            "  class 5 (dog): P=0.7551 R=0.7400 F1=0.7475\n",
            "  class 6 (frog): P=0.7745 R=0.7900 F1=0.7822\n",
            "  class 7 (horse): P=0.8681 R=0.7900 F1=0.8272\n",
            "  class 8 (ship): P=0.8544 R=0.8800 F1=0.8670\n",
            "  class 9 (truck): P=0.8812 R=0.8900 F1=0.8856\n"
          ]
        }
      ],
      "source": [
        "pred_sklearn = gnb_sklearn.predict(test_features_pca)\n",
        "print_eval_report(\n",
        "    \"Gaussian Naive Bayes (scikit-learn, PCA-50)\",\n",
        "    test_labels, pred_sklearn,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "CRyK4M0H8WbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_gnb_sklearn = metrics_row(test_labels, pred_sklearn, \" Scikit’s Gaussian Naive Bayes\")"
      ],
      "metadata": {
        "id": "3rXfAGLO8X1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Decision Tree Implementing Gini Impurity"
      ],
      "metadata": {
        "id": "9HAZeE_ZCJeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(labels):\n",
        "    classes, counts = np.unique(labels, return_counts=True)\n",
        "    probs = counts / len(labels) #Probability\n",
        "    return 1 - np.sum(probs ** 2) #Gini impurity formula"
      ],
      "metadata": {
        "id": "VW8Zz2ksCUtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset"
      ],
      "metadata": {
        "id": "5-lJTO29DhTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, y, feature_idx, threshold): #x=features of the dataset, y=labels/targets\n",
        "    left_indices = X[:, feature_idx] <= threshold\n",
        "    right_indices = X[:, feature_idx] > threshold\n",
        "    return X[left_indices], y[left_indices], X[right_indices], y[right_indices]"
      ],
      "metadata": {
        "id": "SeodytsSDjmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best split"
      ],
      "metadata": {
        "id": "g-HI23HLFAKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(X, y):\n",
        "    best_gini = 1\n",
        "    best_feature_idx = None\n",
        "    best_threshold = None\n",
        "\n",
        "    n_features = X.shape[1] #number of features in the dataset\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        thresholds = np.unique(X[:, feature_idx])\n",
        "        for threshold in thresholds:\n",
        "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature_idx, threshold)\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "            gini_left = gini_impurity(y_left)\n",
        "            gini_right = gini_impurity(y_right)\n",
        "            gini_weight = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)\n",
        "\n",
        "            if gini_weight < best_gini:\n",
        "                best_gini = gini_weight\n",
        "                best_feature_idx = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_idx, best_threshold"
      ],
      "metadata": {
        "id": "2S8N1BzjFCo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the decision tree"
      ],
      "metadata": {
        "id": "Uw4Tdy65Haf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, depth=0, max_depth=50):\n",
        "        self.max_depth = max_depth\n",
        "        self.depth = depth\n",
        "        self.feature_idx = None #index that is used to split\n",
        "        self.threshold = None\n",
        "        self.left = None #left child\n",
        "        self.right = None #right child\n",
        "        self.value = None\n",
        "\n",
        "#Creates a new node for that part of the tree\n",
        "def buildTree(X, y, depth=0, max_depth=50):\n",
        "    node = DecisionTree(depth, max_depth)\n",
        "\n",
        "    #stop condition (pure node or max depth reaqched)\n",
        "    if len(np.unique(y)) == 1 or depth >= max_depth:\n",
        "        node.value = np.bincount(y).argmax()\n",
        "        return node\n",
        "\n",
        "    #best split (picks feature and threashold with lowest Gini impurity)\n",
        "    feature_idx, threshold = best_split(X, y)\n",
        "    if feature_idx is None:\n",
        "        node.value = np.bincount(y).argmax()\n",
        "        return node\n",
        "\n",
        "    node.feature_idx = feature_idx\n",
        "    node.threshold = threshold\n",
        "\n",
        "    X_left, y_left, X_right, y_right = split_dataset(X, y, feature_idx, threshold)\n",
        "    node.left = buildTree(X_left, y_left, depth + 1, max_depth)\n",
        "    node.right = buildTree(X_right, y_right, depth + 1, max_depth)\n",
        "\n",
        "    return node"
      ],
      "metadata": {
        "id": "Vz4us0rIHdYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction of Decision Tree"
      ],
      "metadata": {
        "id": "QJ2qqaTVNAA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(node, X):\n",
        "    y_prediction = []\n",
        "    for x in X: #start at the root node\n",
        "        current = node\n",
        "\n",
        "        #traversing the tree (until leaf is reached)\n",
        "        while current.value is None:\n",
        "            if x[current.feature_idx] <= current.threshold:\n",
        "                current = current.left\n",
        "            else:\n",
        "                current = current.right\n",
        "        y_prediction.append(current.value) #take the class label as prediction if leaf node is reached\n",
        "    return np.array(y_prediction) #returns all the predictions"
      ],
      "metadata": {
        "id": "Vl-5ERzdNGV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics for the Decision Tree"
      ],
      "metadata": {
        "id": "DtxoRU8WOhe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusionMatrix(y_true, y_prediction, num_classes=10):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int) #initializing num_class matrix to zero\n",
        "    for t, p in zip(y_true, y_prediction):\n",
        "        cm[t, p] += 1 #Rows = actual labels, Columns = predicted labels\n",
        "    return cm\n",
        "\n",
        "def computeMetrics(cm):\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    precision = np.diag(cm) / np.maximum(cm.sum(axis=0), 1)\n",
        "    recall = np.diag(cm) / np.maximum(cm.sum(axis=1), 1)\n",
        "    f1 = 2 * precision * recall / np.maximum(precision + recall, 1e-6)\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "zu37Ii_5Qgs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing"
      ],
      "metadata": {
        "id": "kuRmeloM7sC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"decision_tree_scratch.pkl\"\n",
        "\n",
        "#Train if not saved (Saving model)\n",
        "if not os.path.exists(model_path):\n",
        "  tree = buildTree(train_features_pca, train_labels, max_depth=10)\n",
        "  with open(model_path, \"wb\") as f:\n",
        "    pickle.dump(tree, f)\n",
        "else:\n",
        "  with open(model_path, \"rb\") as f:\n",
        "    tree = pickle.load(f)\n",
        "\n",
        "#Evaluation\n",
        "y_prediction = predict(tree, test_features_pca)\n",
        "\n",
        "cm = confusionMatrix(test_labels, y_prediction)\n",
        "\n",
        "accuracy, precision, recall, f1 = computeMetrics(cm)\n",
        "\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== Decision Tree (Scratch, PCA-50) =====\")\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "\n",
        "macro_precision = np.mean(precision)\n",
        "macro_recall = np.mean(recall)\n",
        "macro_f1 = np.mean(f1)\n",
        "\n",
        "print(\"\\nOverall:\")\n",
        "print(f\"- Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"- Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"- Macro Recall   : {macro_recall:.4f}\")\n",
        "print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  class {i} ({name}): \"\n",
        "          f\"P={precision[i]:.4f} R={recall[i]:.4f} F1={f1[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4C_0qC77_vn",
        "outputId": "cc1c9e0f-469c-41a4-ec66-10ccbfc145bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Decision Tree (Scratch, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[53  5 10  6  1  1  0  1 17  6]\n",
            " [ 8 71  2  1  0  1  2  0  4 11]\n",
            " [ 3  0 45 16  9 10 10  6  1  0]\n",
            " [ 2  1  9 54  3 20  7  4  0  0]\n",
            " [ 7  0  8  6 53  9  3 13  0  1]\n",
            " [ 0  0  5 22  4 62  1  4  2  0]\n",
            " [ 5  0  5 10  2  3 73  1  1  0]\n",
            " [ 1  1  8 13  7 13  0 55  1  1]\n",
            " [18  5  2  1  0  0  0  1 66  7]\n",
            " [ 4  9  0  2  1  0  0  1 10 73]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.6050\n",
            "- Macro Precision: 0.6155\n",
            "- Macro Recall   : 0.6050\n",
            "- Macro F1       : 0.6077\n",
            "  class 0 (airplane): P=0.5248 R=0.5300 F1=0.5274\n",
            "  class 1 (automobile): P=0.7717 R=0.7100 F1=0.7396\n",
            "  class 2 (bird): P=0.4787 R=0.4500 F1=0.4639\n",
            "  class 3 (cat): P=0.4122 R=0.5400 F1=0.4675\n",
            "  class 4 (deer): P=0.6625 R=0.5300 F1=0.5889\n",
            "  class 5 (dog): P=0.5210 R=0.6200 F1=0.5662\n",
            "  class 6 (frog): P=0.7604 R=0.7300 F1=0.7449\n",
            "  class 7 (horse): P=0.6395 R=0.5500 F1=0.5914\n",
            "  class 8 (ship): P=0.6471 R=0.6600 F1=0.6535\n",
            "  class 9 (truck): P=0.7374 R=0.7300 F1=0.7337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "zGeuBrlDkDus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_dt_scratch = metrics_row(test_labels, y_prediction, \"Decision Tree (Scratch)\")"
      ],
      "metadata": {
        "id": "tJ9I_UJCkFy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn Decision Tree"
      ],
      "metadata": {
        "id": "AF2YRcZSVd_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"decision_tree_sklearn.pkl\"\n",
        "\n",
        "#Train if not saved (Saving model)\n",
        "if not os.path.exists(model_path):\n",
        "  clf = DecisionTreeClassifier(criterion = 'gini', max_depth=10, random_state=42)\n",
        "  clf.fit(train_features_pca, train_labels)\n",
        "  dump(clf, model_path)\n",
        "else:\n",
        "  clf = load(model_path)\n",
        "\n",
        "y_prediction_sklearn = clf.predict(test_features_pca)\n",
        "\n",
        "#computing matrix from sklearn\n",
        "cm_sklearn = confusion_matrix(test_labels, y_prediction_sklearn)\n",
        "accuracy_sklearn = accuracy_score(test_labels, y_prediction_sklearn)\n",
        "precision_sklearn = precision_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "recall_sklearn = recall_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "f1_sklearn = f1_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "\n",
        "#printing information\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== Decision Tree (Scikit-learn, PCA-50) =====\")\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm_sklearn)\n",
        "\n",
        "macro_precision = np.mean(precision_sklearn)\n",
        "macro_recall = np.mean(recall_sklearn)\n",
        "macro_f1 = np.mean(f1_sklearn)\n",
        "\n",
        "print(\"\\nOverall:\")\n",
        "print(f\"- Accuracy       : {accuracy_sklearn:.4f}\")\n",
        "print(f\"- Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"- Macro Recall   : {macro_recall:.4f}\")\n",
        "print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  class {i} ({name}): \"\n",
        "          f\"P={precision_sklearn[i]:.4f} R={recall_sklearn[i]:.4f} F1={f1_sklearn[i]:.4f}\")"
      ],
      "metadata": {
        "id": "EqtkSc0cVivz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc6b1f7-15ce-400e-97f7-86c9eb3dcb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Decision Tree (Scikit-learn, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[52  5  9  5  3  1  0  3 17  5]\n",
            " [ 6 72  2  1  0  2  2  0  6  9]\n",
            " [ 3  0 45 15  6 10 12  8  1  0]\n",
            " [ 3  1  9 54  2 21  6  3  0  1]\n",
            " [ 6  0  8  6 54  8  3 14  0  1]\n",
            " [ 0  0  6 22  3 62  2  5  0  0]\n",
            " [ 4  0  6 10  5  3 70  2  0  0]\n",
            " [ 1  1  6 13  8 10  0 59  1  1]\n",
            " [20  4  3  1  0  0  1  1 65  5]\n",
            " [ 5 10  0  2  1  0  0  1  7 74]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.6070\n",
            "- Macro Precision: 0.6165\n",
            "- Macro Recall   : 0.6070\n",
            "- Macro F1       : 0.6098\n",
            "  class 0 (airplane): P=0.5200 R=0.5200 F1=0.5200\n",
            "  class 1 (automobile): P=0.7742 R=0.7200 F1=0.7461\n",
            "  class 2 (bird): P=0.4787 R=0.4500 F1=0.4639\n",
            "  class 3 (cat): P=0.4186 R=0.5400 F1=0.4716\n",
            "  class 4 (deer): P=0.6585 R=0.5400 F1=0.5934\n",
            "  class 5 (dog): P=0.5299 R=0.6200 F1=0.5714\n",
            "  class 6 (frog): P=0.7292 R=0.7000 F1=0.7143\n",
            "  class 7 (horse): P=0.6146 R=0.5900 F1=0.6020\n",
            "  class 8 (ship): P=0.6701 R=0.6500 F1=0.6599\n",
            "  class 9 (truck): P=0.7708 R=0.7400 F1=0.7551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "_3eLNmht8HnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_dt_sklearn = metrics_row(test_labels, y_prediction_sklearn, \"Scikit’s implementation of a Decision Tree\")"
      ],
      "metadata": {
        "id": "rmb0qX3e8JiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "kFE8yV1wQQTc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "498c1f20"
      },
      "source": [
        "## 5.1: Define the MLP architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d2c72c1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 512)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.bn_2 = nn.BatchNorm1d(512)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867e0c3d"
      },
      "source": [
        "Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b2c3935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a168c2d6-8804-4d55-9416-b47d198b226a"
      },
      "source": [
        "model = MLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "print(\"Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"Training with PyTorch's CrossEntropyLoss\")\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "print(\"Using SGD optimizer with momentum of 0.9\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multilayer Perceptron (MLP) created\n",
            "Training with PyTorch's CrossEntropyLoss\n",
            "Using SGD optimizer with momentum of 0.9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate confusion matrix for initial MLP"
      ],
      "metadata": {
        "id": "bjIHvwZSYVr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert test data to PyTorch tensors\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_features_pca_tensor)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "# Convert predictions to numpy array\n",
        "pred_mlp_initial = predicted.numpy()\n",
        "\n",
        "# Lines to display the results (Confusion Matrix and metrics)\n",
        "print_eval_report(\n",
        "    \"MLP (Initial, PCA-50)\",\n",
        "    test_labels, pred_mlp_initial,\n",
        "    class_names=cifar10_classes\n",
        ")\n",
        "\n",
        "# Lines to append to the general table (commented out)\n",
        "# mlp_row = metrics_row(test_labels, pred_mlp_initial, \"MLP (Initial, PCA-50)\")\n",
        "# rows.append(mlp_row)\n",
        "# df_results_mlp = pd.DataFrame(rows)\n",
        "# display(df_results_mlp)"
      ],
      "metadata": {
        "id": "GOlX40ikYVRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e893acdd-4010-4f64-9373-0dcf193db8b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== MLP (Initial, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[ 7  6  3  0 33  1  0 34  4 12]\n",
            " [ 2 10 39  0 29  0  0  2  0 18]\n",
            " [ 0 24  5  0  2  1  1 60  5  2]\n",
            " [ 0  9 13  0  0  0  1 62  4 11]\n",
            " [ 0 11 41  0  3  0  1 29  7  8]\n",
            " [ 1 10 23  0  3  0  1 44  5 13]\n",
            " [ 0 18 14  0  5  0  9 41  6  7]\n",
            " [ 0 23 21  0  2  2  1 34  5 12]\n",
            " [ 2 11 24  1 31  1  0  9  2 19]\n",
            " [ 0  4 46  0 30  3  0 10  2  5]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.0750\n",
            "- Macro Precision: 0.1550\n",
            "- Macro Recall   : 0.0750\n",
            "- Macro F1       : 0.0664\n",
            "  class 0 (airplane): P=0.5833 R=0.0700 F1=0.1250\n",
            "  class 1 (automobile): P=0.0794 R=0.1000 F1=0.0885\n",
            "  class 2 (bird): P=0.0218 R=0.0500 F1=0.0304\n",
            "  class 3 (cat): P=0.0000 R=0.0000 F1=0.0000\n",
            "  class 4 (deer): P=0.0217 R=0.0300 F1=0.0252\n",
            "  class 5 (dog): P=0.0000 R=0.0000 F1=0.0000\n",
            "  class 6 (frog): P=0.6429 R=0.0900 F1=0.1579\n",
            "  class 7 (horse): P=0.1046 R=0.3400 F1=0.1600\n",
            "  class 8 (ship): P=0.0500 R=0.0200 F1=0.0286\n",
            "  class 9 (truck): P=0.0467 R=0.0500 F1=0.0483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "Llh6jVPU85L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_initial = metrics_row(test_labels, pred_mlp_initial, \"MLP\")"
      ],
      "metadata": {
        "id": "woVtd49w86nO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.1: Adding layers"
      ],
      "metadata": {
        "id": "9E38fAEmUlqx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D20rIrBwUNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9ed791-5921-4724-ccf9-f81f11f63fdf"
      },
      "source": [
        "# Define an MLP with more layers\n",
        "class DeeperMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(DeeperMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 512)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.bn_2 = nn.BatchNorm1d(512)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(512, 256) # Added a new hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.layer_4 = nn.Linear(256, num_classes) # Output layer adjusted\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        return x\n",
        "\n",
        "print(\"Deeper Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the deeper MLP model\n",
        "deeper_model = DeeperMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "# Define Loss Function and Optimizer for the deeper model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_deeper = optim.SGD(deeper_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Simple training loop (for demonstration - you might need a more robust one)\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Starting training for Deeper MLP...\")\n",
        "for epoch in range(num_epochs):\n",
        "    deeper_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_deeper.zero_grad()\n",
        "        outputs = deeper_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_deeper.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the deeper model\n",
        "deeper_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = deeper_model(test_features_pca_tensor)\n",
        "    _, predicted_deeper = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the deeper MLP\n",
        "pred_deeper_mlp = predicted_deeper.numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Deeper, PCA-50)\",\n",
        "    test_labels, pred_deeper_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")\n",
        "\n",
        "# Add results to combined table (commented for now)\n",
        "# rows.append(metrics_row(test_labels, pred_deeper_mlp, \"MLP (Deeper, PCA-50)\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeper Multilayer Perceptron (MLP) created\n",
            "Starting training for Deeper MLP...\n",
            "Epoch 1/10, Loss: 2.0487\n",
            "Epoch 2/10, Loss: 1.3802\n",
            "Epoch 3/10, Loss: 0.9311\n",
            "Epoch 4/10, Loss: 0.7263\n",
            "Epoch 5/10, Loss: 0.6252\n",
            "Epoch 6/10, Loss: 0.5619\n",
            "Epoch 7/10, Loss: 0.5177\n",
            "Epoch 8/10, Loss: 0.4807\n",
            "Epoch 9/10, Loss: 0.4475\n",
            "Epoch 10/10, Loss: 0.4267\n",
            "Training finished.\n",
            "\n",
            "===== MLP (Deeper, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[80  1  2  2  0  0  0  0 12  3]\n",
            " [ 2 94  0  1  0  0  0  0  1  2]\n",
            " [ 4  0 70  6  4  7  8  0  1  0]\n",
            " [ 1  0  2 76  1 11  8  1  0  0]\n",
            " [ 2  0  2  8 79  1  1  5  2  0]\n",
            " [ 0  0  4 15  1 75  3  1  1  0]\n",
            " [ 1  0  1  4  3  2 89  0  0  0]\n",
            " [ 1  0  0  5 10  4  0 79  1  0]\n",
            " [ 4  0  1  0  0  0  0  0 93  2]\n",
            " [ 2  6  0  0  0  0  1  0  2 89]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8240\n",
            "- Macro Precision: 0.8293\n",
            "- Macro Recall   : 0.8240\n",
            "- Macro F1       : 0.8244\n",
            "  class 0 (airplane): P=0.8247 R=0.8000 F1=0.8122\n",
            "  class 1 (automobile): P=0.9307 R=0.9400 F1=0.9353\n",
            "  class 2 (bird): P=0.8537 R=0.7000 F1=0.7692\n",
            "  class 3 (cat): P=0.6496 R=0.7600 F1=0.7005\n",
            "  class 4 (deer): P=0.8061 R=0.7900 F1=0.7980\n",
            "  class 5 (dog): P=0.7500 R=0.7500 F1=0.7500\n",
            "  class 6 (frog): P=0.8091 R=0.8900 F1=0.8476\n",
            "  class 7 (horse): P=0.9186 R=0.7900 F1=0.8495\n",
            "  class 8 (ship): P=0.8230 R=0.9300 F1=0.8732\n",
            "  class 9 (truck): P=0.9271 R=0.8900 F1=0.9082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "YOsIBx4R9GWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_deeper = metrics_row(test_labels, pred_deeper_mlp, \"MLP (Deeper, PCA-50)\")"
      ],
      "metadata": {
        "id": "tV5maTPf9H_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2: Removing layers"
      ],
      "metadata": {
        "id": "iffaHBamU2N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with fewer layers (e.g., one hidden layer)\n",
        "class ShallowerMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ShallowerMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 256) # Reduced hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(256, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Shallower Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the shallower MLP model\n",
        "shallower_model = ShallowerMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "# Define Loss Function and Optimizer for the shallower model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_shallower = optim.SGD(shallower_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Simple training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Starting training for Shallower MLP...\")\n",
        "for epoch in range(num_epochs):\n",
        "    shallower_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_shallower.zero_grad()\n",
        "        outputs = shallower_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_shallower.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the shallower model\n",
        "shallower_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = shallower_model(test_features_pca_tensor)\n",
        "    _, predicted_shallower = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the shallower MLP\n",
        "pred_shallower_mlp = predicted_shallower.numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Shallower, PCA-50)\",\n",
        "    test_labels, pred_shallower_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")\n",
        "\n",
        "# Add results to combined table (commented for now)\n",
        "# rows.append(metrics_row(test_labels, pred_shallower_mlp, \"MLP (Shallower, PCA-50)\"))"
      ],
      "metadata": {
        "id": "jOjmSVAdkjr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58deabcb-67dc-4704-9722-dbca07f616a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shallower Multilayer Perceptron (MLP) created\n",
            "Starting training for Shallower MLP...\n",
            "Epoch 1/10, Loss: 1.8510\n",
            "Epoch 2/10, Loss: 1.1006\n",
            "Epoch 3/10, Loss: 0.8526\n",
            "Epoch 4/10, Loss: 0.7276\n",
            "Epoch 5/10, Loss: 0.6612\n",
            "Epoch 6/10, Loss: 0.6116\n",
            "Epoch 7/10, Loss: 0.5805\n",
            "Epoch 8/10, Loss: 0.5510\n",
            "Epoch 9/10, Loss: 0.5303\n",
            "Epoch 10/10, Loss: 0.5160\n",
            "Training finished.\n",
            "\n",
            "===== MLP (Shallower, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[81  2  3  0  0  0  1  0  8  5]\n",
            " [ 2 86  0  1  0  0  1  0  2  8]\n",
            " [ 6  0 73  5  3  7  5  0  1  0]\n",
            " [ 1  0  3 72  1 16  7  0  0  0]\n",
            " [ 2  0  2  5 77  4  1  8  1  0]\n",
            " [ 0  0  4 16  3 72  2  2  1  0]\n",
            " [ 1  0  4  2  1  2 89  0  1  0]\n",
            " [ 1  1  1  4  9  5  0 78  1  0]\n",
            " [ 9  1  1  0  0  0  0  0 86  3]\n",
            " [ 2  4  0  1  0  0  0  1  4 88]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8020\n",
            "- Macro Precision: 0.8047\n",
            "- Macro Recall   : 0.8020\n",
            "- Macro F1       : 0.8024\n",
            "  class 0 (airplane): P=0.7714 R=0.8100 F1=0.7902\n",
            "  class 1 (automobile): P=0.9149 R=0.8600 F1=0.8866\n",
            "  class 2 (bird): P=0.8022 R=0.7300 F1=0.7644\n",
            "  class 3 (cat): P=0.6792 R=0.7200 F1=0.6990\n",
            "  class 4 (deer): P=0.8191 R=0.7700 F1=0.7938\n",
            "  class 5 (dog): P=0.6792 R=0.7200 F1=0.6990\n",
            "  class 6 (frog): P=0.8396 R=0.8900 F1=0.8641\n",
            "  class 7 (horse): P=0.8764 R=0.7800 F1=0.8254\n",
            "  class 8 (ship): P=0.8190 R=0.8600 F1=0.8390\n",
            "  class 9 (truck): P=0.8462 R=0.8800 F1=0.8627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "B9richDg9Uk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_shallower = metrics_row(test_labels, pred_shallower_mlp, \"MLP (Shallower, PCA-50)\")"
      ],
      "metadata": {
        "id": "41UXRO6B9Wm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.1: Larger Hidden Layer"
      ],
      "metadata": {
        "id": "NsYWt-gPVEhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with larger hidden layers (e.g., 1024 units)\n",
        "class WiderMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(WiderMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 1024) # Increased hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(1024, 1024) # Increased hidden layer size\n",
        "        self.bn_2 = nn.BatchNorm1d(1024)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(1024, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "print(\"Wider Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the wider MLP model\n",
        "wider_model = WiderMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "# Define Loss Function and Optimizer for the wider model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_wider = optim.SGD(wider_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Simple training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Starting training for Wider MLP...\")\n",
        "for epoch in range(num_epochs):\n",
        "    wider_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_wider.zero_grad()\n",
        "        outputs = wider_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_wider.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the wider model\n",
        "wider_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = wider_model(test_features_pca_tensor)\n",
        "    _, predicted_wider = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the wider MLP\n",
        "pred_wider_mlp = predicted_wider.numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Wider, PCA-50)\",\n",
        "    test_labels, pred_wider_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")\n",
        "\n",
        "# Add results to combined table (commented for now)\n",
        "# rows.append(metrics_row(test_labels, pred_wider_mlp, \"MLP (Wider, PCA-50)\"))"
      ],
      "metadata": {
        "id": "0sKYJiLqlM0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ba8dfd-c5e1-4a7d-e6db-6c9b763bc20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wider Multilayer Perceptron (MLP) created\n",
            "Starting training for Wider MLP...\n",
            "Epoch 1/10, Loss: 1.1657\n",
            "Epoch 2/10, Loss: 0.5947\n",
            "Epoch 3/10, Loss: 0.4998\n",
            "Epoch 4/10, Loss: 0.4503\n",
            "Epoch 5/10, Loss: 0.4059\n",
            "Epoch 6/10, Loss: 0.3846\n",
            "Epoch 7/10, Loss: 0.3572\n",
            "Epoch 8/10, Loss: 0.3322\n",
            "Epoch 9/10, Loss: 0.3176\n",
            "Epoch 10/10, Loss: 0.3102\n",
            "Training finished.\n",
            "\n",
            "===== MLP (Wider, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[86  0  3  0  0  0  2  0  6  3]\n",
            " [ 2 93  0  1  0  0  0  0  0  4]\n",
            " [ 5  0 78  5  2  4  6  0  0  0]\n",
            " [ 1  0  2 77  2  8  8  2  0  0]\n",
            " [ 2  0  2  6 79  1  1  8  1  0]\n",
            " [ 0  0  6 16  2 70  2  3  1  0]\n",
            " [ 1  0  2  4  2  1 89  0  1  0]\n",
            " [ 1  0  0  4  7  4  0 84  0  0]\n",
            " [ 4  0  1  1  0  0  0  0 93  1]\n",
            " [ 2  2  0  2  0  0  0  0  1 93]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8420\n",
            "- Macro Precision: 0.8449\n",
            "- Macro Recall   : 0.8420\n",
            "- Macro F1       : 0.8423\n",
            "  class 0 (airplane): P=0.8269 R=0.8600 F1=0.8431\n",
            "  class 1 (automobile): P=0.9789 R=0.9300 F1=0.9538\n",
            "  class 2 (bird): P=0.8298 R=0.7800 F1=0.8041\n",
            "  class 3 (cat): P=0.6638 R=0.7700 F1=0.7130\n",
            "  class 4 (deer): P=0.8404 R=0.7900 F1=0.8144\n",
            "  class 5 (dog): P=0.7955 R=0.7000 F1=0.7447\n",
            "  class 6 (frog): P=0.8241 R=0.8900 F1=0.8558\n",
            "  class 7 (horse): P=0.8660 R=0.8400 F1=0.8528\n",
            "  class 8 (ship): P=0.9029 R=0.9300 F1=0.9163\n",
            "  class 9 (truck): P=0.9208 R=0.9300 F1=0.9254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "FXoC9J9N9eTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_wider = metrics_row(test_labels, pred_wider_mlp, \"MLP (Wider, PCA-50)\")"
      ],
      "metadata": {
        "id": "BlCgegvK9fvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.2: Smaller Hidden Layer"
      ],
      "metadata": {
        "id": "hghMs375VHO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with smaller hidden layers (e.g., 128 units)\n",
        "class NarrowerMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NarrowerMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 128) # Decreased hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(128, 128) # Decreased hidden layer size\n",
        "        self.bn_2 = nn.BatchNorm1d(128)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(128, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "print(\"Narrower Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the narrower MLP model\n",
        "narrower_model = NarrowerMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "# Define Loss Function and Optimizer for the narrower model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_narrower = optim.SGD(narrower_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors (uncommented for clarity and independence)\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Simple training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "print(\"Starting training for Narrower MLP...\")\n",
        "for epoch in range(num_epochs):\n",
        "    narrower_model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer_narrower.zero_grad()\n",
        "        outputs = narrower_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_narrower.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# Evaluate the narrower model\n",
        "narrower_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = narrower_model(test_features_pca_tensor)\n",
        "    _, predicted_narrower = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the narrower MLP\n",
        "pred_narrower_mlp = predicted_narrower.numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Narrower, PCA-50)\",\n",
        "    test_labels, pred_narrower_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")\n",
        "\n",
        "# Add results to combined table (commented for now)\n",
        "# rows.append(metrics_row(test_labels, pred_narrower_mlp, \"MLP (Narrower, PCA-50)\"))"
      ],
      "metadata": {
        "id": "HevvNWFGm0SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874d2439-c15b-49ba-933d-57013e69acf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Narrower Multilayer Perceptron (MLP) created\n",
            "Starting training for Narrower MLP...\n",
            "Epoch 1/10, Loss: 1.9557\n",
            "Epoch 2/10, Loss: 1.2973\n",
            "Epoch 3/10, Loss: 1.0012\n",
            "Epoch 4/10, Loss: 0.8427\n",
            "Epoch 5/10, Loss: 0.7418\n",
            "Epoch 6/10, Loss: 0.6750\n",
            "Epoch 7/10, Loss: 0.6266\n",
            "Epoch 8/10, Loss: 0.5856\n",
            "Epoch 9/10, Loss: 0.5543\n",
            "Epoch 10/10, Loss: 0.5261\n",
            "Training finished.\n",
            "\n",
            "===== MLP (Narrower, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[80  0  1  2  1  0  1  0 11  4]\n",
            " [ 3 91  0  1  0  0  0  0  1  4]\n",
            " [ 8  1 64  6  4  8  8  1  0  0]\n",
            " [ 1  0  2 73  3 13  7  1  0  0]\n",
            " [ 2  0  1  4 80  3  1  8  1  0]\n",
            " [ 0  0  3 10  5 78  2  1  1  0]\n",
            " [ 1  0  3  4  3  2 86  0  1  0]\n",
            " [ 0  1  1  3  6  4  0 84  0  1]\n",
            " [ 6  1  1  0  0  0  0  0 90  2]\n",
            " [ 3  3  0  2  0  0  1  0  2 89]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8150\n",
            "- Macro Precision: 0.8178\n",
            "- Macro Recall   : 0.8150\n",
            "- Macro F1       : 0.8146\n",
            "  class 0 (airplane): P=0.7692 R=0.8000 F1=0.7843\n",
            "  class 1 (automobile): P=0.9381 R=0.9100 F1=0.9239\n",
            "  class 2 (bird): P=0.8421 R=0.6400 F1=0.7273\n",
            "  class 3 (cat): P=0.6952 R=0.7300 F1=0.7122\n",
            "  class 4 (deer): P=0.7843 R=0.8000 F1=0.7921\n",
            "  class 5 (dog): P=0.7222 R=0.7800 F1=0.7500\n",
            "  class 6 (frog): P=0.8113 R=0.8600 F1=0.8350\n",
            "  class 7 (horse): P=0.8842 R=0.8400 F1=0.8615\n",
            "  class 8 (ship): P=0.8411 R=0.9000 F1=0.8696\n",
            "  class 9 (truck): P=0.8900 R=0.8900 F1=0.8900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "5Q42l-8y9wgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_narrower = metrics_row(test_labels, pred_narrower_mlp, \"MLP (Narrower, PCA-50)\")"
      ],
      "metadata": {
        "id": "5vGfHsky9x0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "f6cpBCyO-EHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1: Implementing and training a VGG11 net"
      ],
      "metadata": {
        "id": "xZhvFqjJ-z-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `torch.nn.CrossEntropyLoss`, and optimize using SGD optimizer with `momentum=0.9`"
      ],
      "metadata": {
        "id": "4Y0LqrKg-9Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing VGG11 - according to assignment description"
      ],
      "metadata": {
        "id": "9r5tQFjCpTTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11, self).__init__()\n",
        "    #extracting image features\n",
        "    self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "axdbMIY2pW7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Data for CNN (resizing to 32x32)"
      ],
      "metadata": {
        "id": "JE82L2Gm1ngj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "39lqp42S1wtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the CNN"
      ],
      "metadata": {
        "id": "p4spd-igBW1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11(num_classes=10).to(device)\n",
        "\n",
        "#where to store the model\n",
        "model_path = \"vgg11_base.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vgg11_base.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "#Train from scratch\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeTNSVGYBY5z",
        "outputId": "1449cb0f-eaf2-42da-e8fb-ff5c0c95f655",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.1291\n",
            "Epoch [2/10] - Loss: 1.6786\n",
            "Epoch [3/10] - Loss: 1.3879\n",
            "Epoch [4/10] - Loss: 1.1219\n",
            "Epoch [5/10] - Loss: 0.8563\n",
            "Epoch [6/10] - Loss: 0.6547\n",
            "Epoch [7/10] - Loss: 0.3824\n",
            "Epoch [8/10] - Loss: 0.2169\n",
            "Epoch [9/10] - Loss: 0.1597\n",
            "Epoch [10/10] - Loss: 0.1243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of CNN"
      ],
      "metadata": {
        "id": "U369LpsB61yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn, preds_cnn = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "#collecting metric\n",
        "row_cnn = metrics_row(labels_cnn, preds_cnn, \"CNN (VGG11)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9y5DMe63oj",
        "outputId": "5cfb4739-b2ca-418c-9bbc-484eb22f9189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[71  4  4  0  2  0  1  1 17  0]\n",
            " [ 3 93  0  0  0  0  0  0  3  1]\n",
            " [18  1 42  4 15 10  5  2  3  0]\n",
            " [ 9  2 10 27 18 15 13  4  2  0]\n",
            " [10  1 10  3 59  4  4  8  1  0]\n",
            " [ 4  3 14 16  9 44  1  6  3  0]\n",
            " [ 2  6  7  2 12  1 68  0  2  0]\n",
            " [11  0  1  4 23  5  0 54  1  1]\n",
            " [18 10  1  1  0  0  1  0 69  0]\n",
            " [ 8 47  1  1  3  1  1  5 16 17]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5440\n",
            "- Macro Precision: 0.5801\n",
            "- Macro Recall   : 0.5440\n",
            "- Macro F1       : 0.5241\n",
            "  class 0 (airplane): P=0.4610 R=0.7100 F1=0.5591\n",
            "  class 1 (automobile): P=0.5569 R=0.9300 F1=0.6966\n",
            "  class 2 (bird): P=0.4667 R=0.4200 F1=0.4421\n",
            "  class 3 (cat): P=0.4655 R=0.2700 F1=0.3418\n",
            "  class 4 (deer): P=0.4184 R=0.5900 F1=0.4896\n",
            "  class 5 (dog): P=0.5500 R=0.4400 F1=0.4889\n",
            "  class 6 (frog): P=0.7234 R=0.6800 F1=0.7010\n",
            "  class 7 (horse): P=0.6750 R=0.5400 F1=0.6000\n",
            "  class 8 (ship): P=0.5897 R=0.6900 F1=0.6359\n",
            "  class 9 (truck): P=0.8947 R=0.1700 F1=0.2857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2: Adding convolutional layers"
      ],
      "metadata": {
        "id": "yjibcjsf_XJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_Add(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            # added an extra layer\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # final feature map = 512×2×2 = 2048\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 2 * 2, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "l_N0Dr5fNP8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data after adding a layer"
      ],
      "metadata": {
        "id": "0Iihcw_pxSRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "E7pEnykOxYAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN after adding layer"
      ],
      "metadata": {
        "id": "WiaPwFU42Ia8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_Add(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_add.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_add.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIlcsIX72LB4",
        "outputId": "b6eaa587-8f23-48b5-cd06-14ac1c35f1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.1743\n",
            "Epoch [2/10] - Loss: 1.7558\n",
            "Epoch [3/10] - Loss: 1.4336\n",
            "Epoch [4/10] - Loss: 1.1342\n",
            "Epoch [5/10] - Loss: 0.8868\n",
            "Epoch [6/10] - Loss: 0.6371\n",
            "Epoch [7/10] - Loss: 0.4843\n",
            "Epoch [8/10] - Loss: 0.4207\n",
            "Epoch [9/10] - Loss: 0.3262\n",
            "Epoch [10/10] - Loss: 0.1797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation CNN after adding layer"
      ],
      "metadata": {
        "id": "TWF_eRsIAbPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11-Added, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_add, preds_cnn_add = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_add    = metrics_row(labels_cnn_add, preds_cnn_add, \"CNN (add)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR1ssLzQAdrL",
        "outputId": "d92a6109-1ee5-4170-b89b-3dc68db96b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11-Added, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[62  5  4  3  1  2  2  9  5  7]\n",
            " [ 2 87  0  1  0  0  0  1  2  7]\n",
            " [13  4 21  5  8 36  5  5  2  1]\n",
            " [ 4  7  3 31  1 30 11 10  0  3]\n",
            " [ 6  2  8  7 28 28  5 16  0  0]\n",
            " [ 1  4  6 11  0 61  3 11  2  1]\n",
            " [ 0  3  2 12  3 11 66  2  0  1]\n",
            " [ 1  0  1  2  1 23  0 69  1  2]\n",
            " [25 15  0  3  0  3  1  0 42 11]\n",
            " [ 5 16  0  1  0  3  2  3  1 69]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5360\n",
            "- Macro Precision: 0.5663\n",
            "- Macro Recall   : 0.5360\n",
            "- Macro F1       : 0.5242\n",
            "  class 0 (airplane): P=0.5210 R=0.6200 F1=0.5662\n",
            "  class 1 (automobile): P=0.6084 R=0.8700 F1=0.7160\n",
            "  class 2 (bird): P=0.4667 R=0.2100 F1=0.2897\n",
            "  class 3 (cat): P=0.4079 R=0.3100 F1=0.3523\n",
            "  class 4 (deer): P=0.6667 R=0.2800 F1=0.3944\n",
            "  class 5 (dog): P=0.3096 R=0.6100 F1=0.4108\n",
            "  class 6 (frog): P=0.6947 R=0.6600 F1=0.6769\n",
            "  class 7 (horse): P=0.5476 R=0.6900 F1=0.6106\n",
            "  class 8 (ship): P=0.7636 R=0.4200 F1=0.5419\n",
            "  class 9 (truck): P=0.6765 R=0.6900 F1=0.6832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3: Removing convolutional layers"
      ],
      "metadata": {
        "id": "itWicRDo_dfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_Remove(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11_Remove, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "      nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "      #removed 3 layers\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(512*4*4, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "      nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "      nn.Linear(4096, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "id": "6hptIWaHOepb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data after removing layer"
      ],
      "metadata": {
        "id": "CZO8Nrd5GVpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "hbJJtX0kGZHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN after removing layer"
      ],
      "metadata": {
        "id": "VHnI1VKCHaeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_Remove(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_remove.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_remove.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn6AbKMxI4Nu",
        "outputId": "e9a72067-2b0a-49ae-8db7-40daa903301c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.1968\n",
            "Epoch [2/10] - Loss: 1.8405\n",
            "Epoch [3/10] - Loss: 1.5912\n",
            "Epoch [4/10] - Loss: 1.4139\n",
            "Epoch [5/10] - Loss: 1.2917\n",
            "Epoch [6/10] - Loss: 1.1850\n",
            "Epoch [7/10] - Loss: 1.0598\n",
            "Epoch [8/10] - Loss: 0.9210\n",
            "Epoch [9/10] - Loss: 0.8102\n",
            "Epoch [10/10] - Loss: 0.6758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating CNN after removing layer"
      ],
      "metadata": {
        "id": "deScbokpHsfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11-Remove, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_remove, preds_cnn_remove = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_remove = metrics_row(labels_cnn_remove, preds_cnn_remove, \"CNN (remove)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5ot0a8JHvQf",
        "outputId": "8a4540f7-e66f-4d7f-a051-7e41877250aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11-Remove, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[59  5  4  0  0  5  0  3 14 10]\n",
            " [ 1 82  1  1  0  1  1  1  4  8]\n",
            " [10  4 43  4  8 23  3  3  2  0]\n",
            " [ 5  5 10 24  3 33  6 10  0  4]\n",
            " [ 7  2 21  1 27 22  2 15  1  2]\n",
            " [ 0  1  9  7  2 70  1  8  2  0]\n",
            " [ 0  5 16  4  1  6 66  1  1  0]\n",
            " [ 1  2  2  4  2 16  0 67  3  3]\n",
            " [12  9  0  2  0  2  0  0 64 11]\n",
            " [ 1 13  1  0  0  1  0  2  7 75]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5770\n",
            "- Macro Precision: 0.5948\n",
            "- Macro Recall   : 0.5770\n",
            "- Macro F1       : 0.5669\n",
            "  class 0 (airplane): P=0.6146 R=0.5900 F1=0.6020\n",
            "  class 1 (automobile): P=0.6406 R=0.8200 F1=0.7193\n",
            "  class 2 (bird): P=0.4019 R=0.4300 F1=0.4155\n",
            "  class 3 (cat): P=0.5106 R=0.2400 F1=0.3265\n",
            "  class 4 (deer): P=0.6279 R=0.2700 F1=0.3776\n",
            "  class 5 (dog): P=0.3911 R=0.7000 F1=0.5018\n",
            "  class 6 (frog): P=0.8354 R=0.6600 F1=0.7374\n",
            "  class 7 (horse): P=0.6091 R=0.6700 F1=0.6381\n",
            "  class 8 (ship): P=0.6531 R=0.6400 F1=0.6465\n",
            "  class 9 (truck): P=0.6637 R=0.7500 F1=0.7042\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4: Larger kernel size"
      ],
      "metadata": {
        "id": "U-FpmLSs_hEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel size of 5x5"
      ],
      "metadata": {
        "id": "JTIT3RiJXC_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_kernel5(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11_kernel5, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "            #Chaning the kernel size to 5x5\n",
        "            nn.Conv2d(3, 64, 5, 1, 2), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 5, 1, 2), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 5, 1, 2), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 5, 1, 2), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "06HdLIBcXM56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for kernel 5x5"
      ],
      "metadata": {
        "id": "pf2rI0axX9y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "aLEN59XBYAnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN with 5x5 kernel size"
      ],
      "metadata": {
        "id": "mI6dR3F3YoOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_kernel5(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_k5.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_k5.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NG4Ibsp_Ythe",
        "outputId": "6a376cbc-517e-46dc-e8c1-4c78e276daf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.0645\n",
            "Epoch [2/10] - Loss: 1.6382\n",
            "Epoch [3/10] - Loss: 1.3605\n",
            "Epoch [4/10] - Loss: 1.1716\n",
            "Epoch [5/10] - Loss: 0.9356\n",
            "Epoch [6/10] - Loss: 0.7532\n",
            "Epoch [7/10] - Loss: 0.5631\n",
            "Epoch [8/10] - Loss: 0.4055\n",
            "Epoch [9/10] - Loss: 0.4562\n",
            "Epoch [10/10] - Loss: 0.3220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the CNN with kernel size 5x5"
      ],
      "metadata": {
        "id": "JjjYYtAkYyBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11_Kernel5, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_k5, preds_cnn_k5 = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_k5      = metrics_row(labels_cnn_k5, preds_cnn_k5, \"CNN (Kernel 5x5)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdmkXbhIY0my",
        "outputId": "b2d30391-fa46-4c5f-d52e-345dff48937c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11_Kernel5, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[51 22  0  2  2  2  2 10  5  4]\n",
            " [ 0 93  0  0  1  2  1  2  0  1]\n",
            " [12  7  5  5 13 34  8 13  3  0]\n",
            " [ 3  8  0 17  8 43 10  9  0  2]\n",
            " [ 2  4  0  6 46 10 10 19  1  2]\n",
            " [ 0  2  1  2  2 72  4 16  1  0]\n",
            " [ 0  5  0  7 13  6 66  2  1  0]\n",
            " [ 0  1  0  3  2 11  1 80  0  2]\n",
            " [10 34  0  3  1  1  1  1 48  1]\n",
            " [ 2 40  0  0  0  2  0  8  2 46]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5240\n",
            "- Macro Precision: 0.5916\n",
            "- Macro Recall   : 0.5240\n",
            "- Macro F1       : 0.4926\n",
            "  class 0 (airplane): P=0.6375 R=0.5100 F1=0.5667\n",
            "  class 1 (automobile): P=0.4306 R=0.9300 F1=0.5886\n",
            "  class 2 (bird): P=0.8333 R=0.0500 F1=0.0943\n",
            "  class 3 (cat): P=0.3778 R=0.1700 F1=0.2345\n",
            "  class 4 (deer): P=0.5227 R=0.4600 F1=0.4894\n",
            "  class 5 (dog): P=0.3934 R=0.7200 F1=0.5088\n",
            "  class 6 (frog): P=0.6408 R=0.6600 F1=0.6502\n",
            "  class 7 (horse): P=0.5000 R=0.8000 F1=0.6154\n",
            "  class 8 (ship): P=0.7869 R=0.4800 F1=0.5963\n",
            "  class 9 (truck): P=0.7931 R=0.4600 F1=0.5823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5: Smaller kernel size"
      ],
      "metadata": {
        "id": "m2QfRA_5_qNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kept the kernel size 3x3 as small, which was already implemented above"
      ],
      "metadata": {
        "id": "BwqvygAWdyF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7: Evaluation Table"
      ],
      "metadata": {
        "id": "0W_9adGYnrv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Evaluation Table"
      ],
      "metadata": {
        "id": "IuflfyWmnvLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [\n",
        "    row_gnb_scratch,\n",
        "    row_gnb_sklearn,\n",
        "    row_dt_scratch,\n",
        "    row_dt_sklearn,\n",
        "    row_mlp_initial,\n",
        "    row_mlp_deeper,\n",
        "    row_mlp_shallower,\n",
        "    row_mlp_wider,\n",
        "    row_mlp_narrower,\n",
        "    row_cnn,\n",
        "    row_cnn_add,\n",
        "    row_cnn_remove,\n",
        "    row_cnn_k5\n",
        "]\n",
        "\n",
        "df_results = pd.DataFrame(rows)\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "SBvwv-_WoLQu",
        "outputId": "4d9c7f7e-340b-4b24-cb68-ec81d0d5a859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         Model  Accuracy  Macro Precision  \\\n",
              "0                        Naive Bayes (Scratch)     0.789         0.793704   \n",
              "1                Scikit’s Gaussian Naive Bayes     0.789         0.793704   \n",
              "2                      Decision Tree (Scratch)     0.605         0.615532   \n",
              "3   Scikit’s implementation of a Decision Tree     0.607         0.616466   \n",
              "4                                          MLP     0.075         0.155047   \n",
              "5                         MLP (Deeper, PCA-50)     0.824         0.829258   \n",
              "6                      MLP (Shallower, PCA-50)     0.802         0.804739   \n",
              "7                          MLP (Wider, PCA-50)     0.842         0.844909   \n",
              "8                       MLP (Narrower, PCA-50)     0.815         0.817791   \n",
              "9                                  CNN (VGG11)     0.544         0.580143   \n",
              "10                                   CNN (add)     0.536         0.566274   \n",
              "11                                CNN (remove)     0.577         0.594800   \n",
              "12                            CNN (Kernel 5x5)     0.524         0.591610   \n",
              "\n",
              "    Macro Recall  Macro F1  \n",
              "0          0.789  0.789589  \n",
              "1          0.789  0.789589  \n",
              "2          0.605  0.607692  \n",
              "3          0.607  0.609781  \n",
              "4          0.075  0.066388  \n",
              "5          0.824  0.824366  \n",
              "6          0.802  0.802436  \n",
              "7          0.842  0.842337  \n",
              "8          0.815  0.814577  \n",
              "9          0.544  0.524077  \n",
              "10         0.536  0.524197  \n",
              "11         0.577  0.566896  \n",
              "12         0.524  0.492647  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6956a34c-35f3-401b-867d-8a0325980d20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive Bayes (Scratch)</td>\n",
              "      <td>0.789</td>\n",
              "      <td>0.793704</td>\n",
              "      <td>0.789</td>\n",
              "      <td>0.789589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Scikit’s Gaussian Naive Bayes</td>\n",
              "      <td>0.789</td>\n",
              "      <td>0.793704</td>\n",
              "      <td>0.789</td>\n",
              "      <td>0.789589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree (Scratch)</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.615532</td>\n",
              "      <td>0.605</td>\n",
              "      <td>0.607692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Scikit’s implementation of a Decision Tree</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.616466</td>\n",
              "      <td>0.607</td>\n",
              "      <td>0.609781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.155047</td>\n",
              "      <td>0.075</td>\n",
              "      <td>0.066388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MLP (Deeper, PCA-50)</td>\n",
              "      <td>0.824</td>\n",
              "      <td>0.829258</td>\n",
              "      <td>0.824</td>\n",
              "      <td>0.824366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MLP (Shallower, PCA-50)</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.804739</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.802436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MLP (Wider, PCA-50)</td>\n",
              "      <td>0.842</td>\n",
              "      <td>0.844909</td>\n",
              "      <td>0.842</td>\n",
              "      <td>0.842337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP (Narrower, PCA-50)</td>\n",
              "      <td>0.815</td>\n",
              "      <td>0.817791</td>\n",
              "      <td>0.815</td>\n",
              "      <td>0.814577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CNN (VGG11)</td>\n",
              "      <td>0.544</td>\n",
              "      <td>0.580143</td>\n",
              "      <td>0.544</td>\n",
              "      <td>0.524077</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CNN (add)</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.566274</td>\n",
              "      <td>0.536</td>\n",
              "      <td>0.524197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CNN (remove)</td>\n",
              "      <td>0.577</td>\n",
              "      <td>0.594800</td>\n",
              "      <td>0.577</td>\n",
              "      <td>0.566896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CNN (Kernel 5x5)</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.591610</td>\n",
              "      <td>0.524</td>\n",
              "      <td>0.492647</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6956a34c-35f3-401b-867d-8a0325980d20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6956a34c-35f3-401b-867d-8a0325980d20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6956a34c-35f3-401b-867d-8a0325980d20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-27b5ad66-a52d-4d25-9a49-4ce69ec6c6a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-27b5ad66-a52d-4d25-9a49-4ce69ec6c6a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-27b5ad66-a52d-4d25-9a49-4ce69ec6c6a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_5c0cbfe8-b383-461c-8ae1-c982439639fe\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5c0cbfe8-b383-461c-8ae1-c982439639fe button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"CNN (remove)\",\n          \"CNN (VGG11)\",\n          \"Naive Bayes (Scratch)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21113202213125032,\n        \"min\": 0.075,\n        \"max\": 0.842,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.577,\n          0.536,\n          0.789\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1887096191571238,\n        \"min\": 0.1550473103703711,\n        \"max\": 0.8449089016288915,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.5947996205057012,\n          0.5662735593575122,\n          0.7937040715330635\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21113202213125032,\n        \"min\": 0.07500000000000002,\n        \"max\": 0.842,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.577,\n          0.5359999999999999,\n          0.7889999999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21623900257703021,\n        \"min\": 0.0663876140190463,\n        \"max\": 0.8423374324711481,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.5668958507971575,\n          0.5241974282674456,\n          0.7895885870080211\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E1Q4AmqJQjq6",
        "R1_896xfQtku",
        "ztI1l3YLmJ3a",
        "9HAZeE_ZCJeA",
        "kFE8yV1wQQTc",
        "498c1f20",
        "9E38fAEmUlqx",
        "iffaHBamU2N8",
        "NsYWt-gPVEhj",
        "hghMs375VHO9",
        "m2QfRA_5_qNJ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}