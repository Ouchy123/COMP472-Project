{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 2: Dataset Setup"
      ],
      "metadata": {
        "id": "E1Q4AmqJQjq6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0lkgosKBJWNj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "from joblib import dump, load\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r89_h5D2PQq1"
      },
      "source": [
        "## Resizing images and normalizing them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AGI8TxBRPYCp"
      },
      "outputs": [],
      "source": [
        "transforms.resnet = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE16cKo-Q_GU"
      },
      "source": [
        "### Loading dataset (CIFAR-10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k73ePpbLRHkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b789a6e0-c0e1-43ee-830b-b92b4ba0a01f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "trainset_full = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.resnet)\n",
        "testset_full = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.resnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ua6umg6SWkx"
      },
      "source": [
        "Selecting 500 training images and 100 test images per class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "o6r_VujwSfbA"
      },
      "outputs": [],
      "source": [
        "def get_subset(dataset, indices):\n",
        "    target = np.array(dataset.targets)\n",
        "    selected_indices = []\n",
        "    for i in range(10):\n",
        "        i_indices = np.where(target == i)[0][:indices]\n",
        "        selected_indices.extend(i_indices)\n",
        "    return Subset(dataset, selected_indices)\n",
        "\n",
        "trainset = get_subset(trainset_full, 500)\n",
        "testset = get_subset(testset_full, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1G3867bV2Qb"
      },
      "source": [
        "### Loading pretrained ResNet-18 and removing the last layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "CuNFG3mYWQCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25c5418b-8b75-4201-8a9a-26da820475bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 187MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "resnet18 = models.resnet18(pretrained=True)\n",
        "feature_extractor = torch.nn.Sequential(*list(resnet18.children())[:-1])\n",
        "feature_extractor.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kMc7i6DXlw2"
      },
      "source": [
        "### Extract feature vector to get 512x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "zlJjGfXKa60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45dd31c3-3978-4a42-ffb1-e9619897fdcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 512) (1000, 512)\n"
          ]
        }
      ],
      "source": [
        "def extract_features(dataset, model, batch_size=64):\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, batch_labels in dataloader:\n",
        "            outputs = model(images)\n",
        "            outputs = outputs.view(outputs.size(0), -1)\n",
        "            features.append(outputs)\n",
        "            labels.append(batch_labels)\n",
        "    features = torch.cat(features).numpy()\n",
        "    labels = torch.cat(labels).numpy()\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(trainset, feature_extractor)\n",
        "test_features, test_labels = extract_features(testset, feature_extractor)\n",
        "\n",
        "print(train_features.shape, test_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwRh5vdOd-vT"
      },
      "source": [
        "## Using PCA to reduce the size of feature vector from 512x1 to 50x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdUiyYSaeFl-",
        "outputId": "1135835d-d318-44cb-b5ba-7ca2e224dcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 50) (1000, 50)\n"
          ]
        }
      ],
      "source": [
        "pca = PCA(n_components=50)\n",
        "train_features_pca = pca.fit_transform(train_features)\n",
        "test_features_pca = pca.transform(test_features)\n",
        "\n",
        "print(train_features_pca.shape, test_features_pca.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to extract metrics"
      ],
      "metadata": {
        "id": "yutH1YxsheLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def metrics_row(y_true, y_pred, model_name):\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='macro', zero_division=0\n",
        "    )\n",
        "    return {\n",
        "        \"Model\": model_name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Macro Precision\": prec,\n",
        "        \"Macro Recall\": rec,\n",
        "        \"Macro F1\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "8T-oo3u8hkTC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3: Naive Bayes"
      ],
      "metadata": {
        "id": "R1_896xfQtku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEbqudEgkSz_"
      },
      "source": [
        "Part3.1.1 - Build confusion matrix (rows=true labels, cols=predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-p1SDVCVkJnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def confusion_matrix_np(y_true, y_pred, num_classes=10):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
        "    for t, p in zip(y_true, y_pred):\n",
        "        cm[t, p] += 1\n",
        "    return cm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA2qpAbEkpsc"
      },
      "source": [
        "Part3.1.2 - Compute precision, recall, F1, and accuracy (macro + per-class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wrSfHN6WklkU"
      },
      "outputs": [],
      "source": [
        "def precision_recall_f1_from_cm(cm, eps=1e-12):\n",
        "    tp = np.diag(cm).astype(np.float64)\n",
        "    fp = cm.sum(axis=0) - tp\n",
        "    fn = cm.sum(axis=1) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + eps)\n",
        "    recall    = tp / (tp + fn + eps)\n",
        "    f1        = 2 * precision * recall / (precision + recall + eps)\n",
        "\n",
        "    return {\n",
        "        \"per_class_precision\": precision,\n",
        "        \"per_class_recall\": recall,\n",
        "        \"per_class_f1\": f1,\n",
        "        \"macro_precision\": precision.mean(),\n",
        "        \"macro_recall\": recall.mean(),\n",
        "        \"macro_f1\": f1.mean(),\n",
        "        \"accuracy\": tp.sum() / cm.sum()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQs9SP0slDqh"
      },
      "source": [
        "Part 3.1.3 — Pretty-print evaluation report with metrics and confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P1Jt_q3zk-ev"
      },
      "outputs": [],
      "source": [
        "def print_eval_report(name, y_true, y_pred, class_names=None):\n",
        "    cm = confusion_matrix_np(y_true, y_pred, num_classes=10)\n",
        "    m = precision_recall_f1_from_cm(cm)\n",
        "\n",
        "    print(f\"\\n===== {name} =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {m['accuracy']:.4f}\")\n",
        "    print(f\"- Macro Precision: {m['macro_precision']:.4f}\")\n",
        "    print(f\"- Macro Recall   : {m['macro_recall']:.4f}\")\n",
        "    print(f\"- Macro F1       : {m['macro_f1']:.4f}\")\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [str(i) for i in range(10)]\n",
        "    for i, cname in enumerate(class_names):\n",
        "        p = m[\"per_class_precision\"][i]\n",
        "        r = m[\"per_class_recall\"][i]\n",
        "        f = m[\"per_class_f1\"][i]\n",
        "        print(f\"  class {i} ({cname}): P={p:.4f} R={r:.4f} F1={f:.4f}\")\n",
        "\n",
        "cifar10_classes = [\n",
        "    \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
        "    \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztI1l3YLmJ3a"
      },
      "source": [
        "# Part 3.2.1 — Define Gaussian Naive Bayes class (NumPy only implementation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xYYkJ6SHmHKu"
      },
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n",
        "    \"\"\"\n",
        "    NumPy-only Gaussian Naive Bayes:\n",
        "    - Estimate class priors, per-class means/variances\n",
        "    - Predict via log-likelihood + log-prior (argmax)\n",
        "    \"\"\"\n",
        "    def __init__(self, var_smoothing=1e-9):\n",
        "        self.var_smoothing = var_smoothing\n",
        "        self.classes_ = None\n",
        "        self.class_priors_ = None\n",
        "        self.means_ = None\n",
        "        self.vars_ = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        C = len(self.classes_)\n",
        "        N, D = X.shape\n",
        "\n",
        "        self.means_ = np.zeros((C, D), dtype=np.float64)\n",
        "        self.vars_  = np.zeros((C, D), dtype=np.float64)\n",
        "        self.class_priors_ = np.zeros(C, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            Xc = X[y == c]\n",
        "            self.means_[idx] = Xc.mean(axis=0)\n",
        "            self.vars_[idx]  = Xc.var(axis=0) + self.var_smoothing\n",
        "            self.class_priors_[idx] = len(Xc) / float(N)\n",
        "        return self\n",
        "\n",
        "    def _log_gaussian_likelihood(self, X):\n",
        "        X = np.asarray(X)\n",
        "        means = self.means_[None, :, :]   # (1, C, D)\n",
        "        vars_ = self.vars_[None, :, :]    # (1, C, D)\n",
        "        X_    = X[:, None, :]             # (N, 1, D)\n",
        "\n",
        "        log_term  = -0.5 * (np.log(2.0 * np.pi * vars_)).sum(axis=2)\n",
        "        quad_term = -0.5 * (((X_ - means) ** 2) / vars_).sum(axis=2)\n",
        "        return log_term + quad_term\n",
        "\n",
        "    def predict(self, X):\n",
        "        log_like = self._log_gaussian_likelihood(X)\n",
        "        log_prior = np.log(self.class_priors_)[None, :]\n",
        "        scores = log_like + log_prior\n",
        "        idx = np.argmax(scores, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgGPBc48mSED"
      },
      "source": [
        "Part 3.2.2 — Fit method: estimate means, variances, and priors for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QDrL6Ws0mNAM"
      },
      "outputs": [],
      "source": [
        "def fit(self, X, y):\n",
        "        X = np.asarray(X)\n",
        "        y = np.asarray(y)\n",
        "\n",
        "        self.classes_ = np.unique(y)\n",
        "        C = len(self.classes_)\n",
        "        N, D = X.shape\n",
        "\n",
        "        self.means_ = np.zeros((C, D), dtype=np.float64)\n",
        "        self.vars_  = np.zeros((C, D), dtype=np.float64)\n",
        "        self.class_priors_ = np.zeros(C, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self.classes_):\n",
        "            Xc = X[y == c]\n",
        "            self.means_[idx] = Xc.mean(axis=0)\n",
        "            self.vars_[idx]  = Xc.var(axis=0) + self.var_smoothing\n",
        "            self.class_priors_[idx] = len(Xc) / float(N)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTAWNb20mYde"
      },
      "source": [
        "Part 3.2.3 — Log-likelihood computation and prediction using argmax of scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8mAJzXJCmZO1"
      },
      "outputs": [],
      "source": [
        "def _log_gaussian_likelihood(self, X):\n",
        "        X = np.asarray(X)\n",
        "        means = self.means_[None, :, :]   # (1, C, D)\n",
        "        vars_ = self.vars_[None, :, :]    # (1, C, D)\n",
        "        X_    = X[:, None, :]             # (N, 1, D)\n",
        "\n",
        "        log_term  = -0.5 * (np.log(2.0 * np.pi * vars_)).sum(axis=2)\n",
        "        quad_term = -0.5 * (((X_ - means) ** 2) / vars_).sum(axis=2)\n",
        "        return log_term + quad_term\n",
        "\n",
        "def predict(self, X):\n",
        "        log_like = self._log_gaussian_likelihood(X)\n",
        "        log_prior = np.log(self.class_priors_)[None, :]\n",
        "        scores = log_like + log_prior\n",
        "        idx = np.argmax(scores, axis=1)\n",
        "        return self.classes_[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_M71BKCtF2T"
      },
      "source": [
        "Part 3.3.1 — Fit (train) the scratch GNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Ry4ECkstaK",
        "outputId": "9dde074d-d2b0-40f1-ed60-591b06a577c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scratch GNB fitted on PCA-50 features.\n"
          ]
        }
      ],
      "source": [
        "gnb_scratch = GaussianNaiveBayes(var_smoothing=1e-9)\n",
        "gnb_scratch.fit(train_features_pca, train_labels)\n",
        "print(\"Scratch GNB fitted on PCA-50 features.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcF6dwZ-tOoX"
      },
      "source": [
        "Part 3.3.2 — Predict & print evaluation report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frybcqeLtLXD",
        "outputId": "1fa5adda-3cdb-4edc-e792-8f18676ca2cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Gaussian Naive Bayes (Scratch, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[80  1  0  1  0  0  1  0 13  4]\n",
            " [ 3 88  0  2  1  0  0  0  0  6]\n",
            " [ 7  0 62  8  7  4 11  0  1  0]\n",
            " [ 1  0  3 76  4 10  6  0  0  0]\n",
            " [ 1  0  5  7 74  2  3  7  1  0]\n",
            " [ 0  1  4 14  3 74  2  2  0  0]\n",
            " [ 2  0  5  7  6  0 79  1  0  0]\n",
            " [ 1  1  0  5  6  5  0 81  1  0]\n",
            " [ 7  0  1  0  1  0  0  0 88  3]\n",
            " [ 5  2  0  2  0  0  0  2  1 88]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.7900\n",
            "- Macro Precision: 0.7951\n",
            "- Macro Recall   : 0.7900\n",
            "- Macro F1       : 0.7906\n",
            "  class 0 (airplane): P=0.7477 R=0.8000 F1=0.7729\n",
            "  class 1 (automobile): P=0.9462 R=0.8800 F1=0.9119\n",
            "  class 2 (bird): P=0.7750 R=0.6200 F1=0.6889\n",
            "  class 3 (cat): P=0.6230 R=0.7600 F1=0.6847\n",
            "  class 4 (deer): P=0.7255 R=0.7400 F1=0.7327\n",
            "  class 5 (dog): P=0.7789 R=0.7400 F1=0.7590\n",
            "  class 6 (frog): P=0.7745 R=0.7900 F1=0.7822\n",
            "  class 7 (horse): P=0.8710 R=0.8100 F1=0.8394\n",
            "  class 8 (ship): P=0.8381 R=0.8800 F1=0.8585\n",
            "  class 9 (truck): P=0.8713 R=0.8800 F1=0.8756\n"
          ]
        }
      ],
      "source": [
        "pred_scratch = gnb_scratch.predict(test_features_pca)\n",
        "print_eval_report(\n",
        "    \"Gaussian Naive Bayes (Scratch, PCA-50)\",\n",
        "    test_labels, pred_scratch,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "UuHSVnZDjrbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_gnb_scratch = metrics_row(test_labels, pred_scratch, \"Naive Bayes (Scratch)\")"
      ],
      "metadata": {
        "id": "Uiv_L0CujtwO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAPLkEWktPvB"
      },
      "source": [
        "## Part 3.4.1 — Gaussian Naive Bayes (scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6_h5ACOtP9o",
        "outputId": "c83a6e9f-7b90-40f6-dea7-939bb7181f03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sklearn GNB fitted on PCA-50 features.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb_sklearn = GaussianNB(var_smoothing=1e-9)\n",
        "gnb_sklearn.fit(train_features_pca, train_labels)\n",
        "print(\"sklearn GNB fitted on PCA-50 features.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU0oesJqtq6f"
      },
      "source": [
        "Part 3.4.2 — Predict & print evaluation report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZwIyDdstrM8",
        "outputId": "f47f8207-325a-4df0-b7cb-b953bd678d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Gaussian Naive Bayes (scikit-learn, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[80  1  0  1  0  0  1  0 13  4]\n",
            " [ 3 88  0  2  1  0  0  0  0  6]\n",
            " [ 7  0 62  8  7  4 11  0  1  0]\n",
            " [ 1  0  3 76  4 10  6  0  0  0]\n",
            " [ 1  0  5  7 74  2  3  7  1  0]\n",
            " [ 0  1  4 14  3 74  2  2  0  0]\n",
            " [ 2  0  5  7  6  0 79  1  0  0]\n",
            " [ 1  1  0  5  6  5  0 81  1  0]\n",
            " [ 7  0  1  0  1  0  0  0 88  3]\n",
            " [ 5  2  0  2  0  0  0  2  1 88]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.7900\n",
            "- Macro Precision: 0.7951\n",
            "- Macro Recall   : 0.7900\n",
            "- Macro F1       : 0.7906\n",
            "  class 0 (airplane): P=0.7477 R=0.8000 F1=0.7729\n",
            "  class 1 (automobile): P=0.9462 R=0.8800 F1=0.9119\n",
            "  class 2 (bird): P=0.7750 R=0.6200 F1=0.6889\n",
            "  class 3 (cat): P=0.6230 R=0.7600 F1=0.6847\n",
            "  class 4 (deer): P=0.7255 R=0.7400 F1=0.7327\n",
            "  class 5 (dog): P=0.7789 R=0.7400 F1=0.7590\n",
            "  class 6 (frog): P=0.7745 R=0.7900 F1=0.7822\n",
            "  class 7 (horse): P=0.8710 R=0.8100 F1=0.8394\n",
            "  class 8 (ship): P=0.8381 R=0.8800 F1=0.8585\n",
            "  class 9 (truck): P=0.8713 R=0.8800 F1=0.8756\n"
          ]
        }
      ],
      "source": [
        "pred_sklearn = gnb_sklearn.predict(test_features_pca)\n",
        "print_eval_report(\n",
        "    \"Gaussian Naive Bayes (scikit-learn, PCA-50)\",\n",
        "    test_labels, pred_sklearn,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "CRyK4M0H8WbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_gnb_sklearn = metrics_row(test_labels, pred_sklearn, \" Scikit’s Gaussian Naive Bayes\")"
      ],
      "metadata": {
        "id": "3rXfAGLO8X1q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: Decision Tree Implementing Gini Impurity"
      ],
      "metadata": {
        "id": "9HAZeE_ZCJeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_impurity(labels):\n",
        "    classes, counts = np.unique(labels, return_counts=True)\n",
        "    probs = counts / len(labels) #Probability\n",
        "    return 1 - np.sum(probs ** 2) #Gini impurity formula"
      ],
      "metadata": {
        "id": "VW8Zz2ksCUtX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset"
      ],
      "metadata": {
        "id": "5-lJTO29DhTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, y, feature_idx, threshold): #x=features of the dataset, y=labels/targets\n",
        "    left_indices = X[:, feature_idx] <= threshold\n",
        "    right_indices = X[:, feature_idx] > threshold\n",
        "    return X[left_indices], y[left_indices], X[right_indices], y[right_indices]"
      ],
      "metadata": {
        "id": "SeodytsSDjmx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding the best split"
      ],
      "metadata": {
        "id": "g-HI23HLFAKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_split(X, y):\n",
        "    best_gini = 1\n",
        "    best_feature_idx = None\n",
        "    best_threshold = None\n",
        "\n",
        "    n_features = X.shape[1] #number of features in the dataset\n",
        "\n",
        "    for feature_idx in range(n_features):\n",
        "        thresholds = np.unique(X[:, feature_idx])\n",
        "        for threshold in thresholds:\n",
        "            X_left, y_left, X_right, y_right = split_dataset(X, y, feature_idx, threshold)\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "            gini_left = gini_impurity(y_left)\n",
        "            gini_right = gini_impurity(y_right)\n",
        "            gini_weight = (len(y_left) * gini_left + len(y_right) * gini_right) / len(y)\n",
        "\n",
        "            if gini_weight < best_gini:\n",
        "                best_gini = gini_weight\n",
        "                best_feature_idx = feature_idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_feature_idx, best_threshold"
      ],
      "metadata": {
        "id": "2S8N1BzjFCo0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the decision tree"
      ],
      "metadata": {
        "id": "Uw4Tdy65Haf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "    def __init__(self, depth=0, max_depth=50):\n",
        "        self.max_depth = max_depth\n",
        "        self.depth = depth\n",
        "        self.feature_idx = None #index that is used to split\n",
        "        self.threshold = None\n",
        "        self.left = None #left child\n",
        "        self.right = None #right child\n",
        "        self.value = None\n",
        "\n",
        "#Creates a new node for that part of the tree\n",
        "def buildTree(X, y, depth=0, max_depth=50):\n",
        "    node = DecisionTree(depth, max_depth)\n",
        "\n",
        "    #stop condition (pure node or max depth reaqched)\n",
        "    if len(np.unique(y)) == 1 or depth >= max_depth:\n",
        "        node.value = np.bincount(y).argmax()\n",
        "        return node\n",
        "\n",
        "    #best split (picks feature and threashold with lowest Gini impurity)\n",
        "    feature_idx, threshold = best_split(X, y)\n",
        "    if feature_idx is None:\n",
        "        node.value = np.bincount(y).argmax()\n",
        "        return node\n",
        "\n",
        "    node.feature_idx = feature_idx\n",
        "    node.threshold = threshold\n",
        "\n",
        "    X_left, y_left, X_right, y_right = split_dataset(X, y, feature_idx, threshold)\n",
        "    node.left = buildTree(X_left, y_left, depth + 1, max_depth)\n",
        "    node.right = buildTree(X_right, y_right, depth + 1, max_depth)\n",
        "\n",
        "    return node"
      ],
      "metadata": {
        "id": "Vz4us0rIHdYB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction of Decision Tree"
      ],
      "metadata": {
        "id": "QJ2qqaTVNAA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(node, X):\n",
        "    y_prediction = []\n",
        "    for x in X: #start at the root node\n",
        "        current = node\n",
        "\n",
        "        #traversing the tree (until leaf is reached)\n",
        "        while current.value is None:\n",
        "            if x[current.feature_idx] <= current.threshold:\n",
        "                current = current.left\n",
        "            else:\n",
        "                current = current.right\n",
        "        y_prediction.append(current.value) #take the class label as prediction if leaf node is reached\n",
        "    return np.array(y_prediction) #returns all the predictions"
      ],
      "metadata": {
        "id": "Vl-5ERzdNGV0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metrics for the Decision Tree"
      ],
      "metadata": {
        "id": "DtxoRU8WOhe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def confusionMatrix(y_true, y_prediction, num_classes=10):\n",
        "    cm = np.zeros((num_classes, num_classes), dtype=int) #initializing num_class matrix to zero\n",
        "    for t, p in zip(y_true, y_prediction):\n",
        "        cm[t, p] += 1 #Rows = actual labels, Columns = predicted labels\n",
        "    return cm\n",
        "\n",
        "def computeMetrics(cm):\n",
        "    accuracy = np.trace(cm) / np.sum(cm)\n",
        "    precision = np.diag(cm) / np.maximum(cm.sum(axis=0), 1)\n",
        "    recall = np.diag(cm) / np.maximum(cm.sum(axis=1), 1)\n",
        "    f1 = 2 * precision * recall / np.maximum(precision + recall, 1e-6)\n",
        "    return accuracy, precision, recall, f1"
      ],
      "metadata": {
        "id": "zu37Ii_5Qgs6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing"
      ],
      "metadata": {
        "id": "kuRmeloM7sC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"decision_tree_scratch.pkl\"\n",
        "\n",
        "#Train if not saved (Saving model)\n",
        "if not os.path.exists(model_path):\n",
        "  tree = buildTree(train_features_pca, train_labels, max_depth=10)\n",
        "  with open(model_path, \"wb\") as f:\n",
        "    pickle.dump(tree, f)\n",
        "else:\n",
        "  with open(model_path, \"rb\") as f:\n",
        "    tree = pickle.load(f)\n",
        "\n",
        "#Evaluation\n",
        "y_prediction = predict(tree, test_features_pca)\n",
        "\n",
        "cm = confusionMatrix(test_labels, y_prediction)\n",
        "\n",
        "accuracy, precision, recall, f1 = computeMetrics(cm)\n",
        "\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== Decision Tree (Scratch, PCA-50) =====\")\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "\n",
        "macro_precision = np.mean(precision)\n",
        "macro_recall = np.mean(recall)\n",
        "macro_f1 = np.mean(f1)\n",
        "\n",
        "print(\"\\nOverall:\")\n",
        "print(f\"- Accuracy       : {accuracy:.4f}\")\n",
        "print(f\"- Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"- Macro Recall   : {macro_recall:.4f}\")\n",
        "print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  class {i} ({name}): \"\n",
        "          f\"P={precision[i]:.4f} R={recall[i]:.4f} F1={f1[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4C_0qC77_vn",
        "outputId": "40534972-20f5-4471-fc37-79148d120430"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Decision Tree (Scratch, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[54  7  9  6  1  1  1  1 15  5]\n",
            " [ 4 70  2  3  0  1  3  1  5 11]\n",
            " [ 4  0 42 19  9 11 11  3  1  0]\n",
            " [ 2  0  8 61  2 17  8  1  0  1]\n",
            " [ 3  0  7  9 56  4  2 18  0  1]\n",
            " [ 0  0  6 25  4 58  1  5  1  0]\n",
            " [ 4  0  6 11  2  2 71  2  2  0]\n",
            " [ 2  1  5 12 13 11  0 54  1  1]\n",
            " [18  5  2  0  0  0  2  2 64  7]\n",
            " [ 6 11  0  4  1  0  0  1  8 69]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5990\n",
            "- Macro Precision: 0.6096\n",
            "- Macro Recall   : 0.5990\n",
            "- Macro F1       : 0.6014\n",
            "  class 0 (airplane): P=0.5567 R=0.5400 F1=0.5482\n",
            "  class 1 (automobile): P=0.7447 R=0.7000 F1=0.7216\n",
            "  class 2 (bird): P=0.4828 R=0.4200 F1=0.4492\n",
            "  class 3 (cat): P=0.4067 R=0.6100 F1=0.4880\n",
            "  class 4 (deer): P=0.6364 R=0.5600 F1=0.5957\n",
            "  class 5 (dog): P=0.5524 R=0.5800 F1=0.5659\n",
            "  class 6 (frog): P=0.7172 R=0.7100 F1=0.7136\n",
            "  class 7 (horse): P=0.6136 R=0.5400 F1=0.5745\n",
            "  class 8 (ship): P=0.6598 R=0.6400 F1=0.6497\n",
            "  class 9 (truck): P=0.7263 R=0.6900 F1=0.7077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "zGeuBrlDkDus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_dt_scratch = metrics_row(test_labels, y_prediction, \"Decision Tree (Scratch)\")"
      ],
      "metadata": {
        "id": "tJ9I_UJCkFy6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-Learn Decision Tree"
      ],
      "metadata": {
        "id": "AF2YRcZSVd_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"decision_tree_sklearn.pkl\"\n",
        "\n",
        "#Train if not saved (Saving model)\n",
        "if not os.path.exists(model_path):\n",
        "  clf = DecisionTreeClassifier(criterion = 'gini', max_depth=10, random_state=42)\n",
        "  clf.fit(train_features_pca, train_labels)\n",
        "  dump(clf, model_path)\n",
        "else:\n",
        "  clf = load(model_path)\n",
        "\n",
        "y_prediction_sklearn = clf.predict(test_features_pca)\n",
        "\n",
        "#computing matrix from sklearn\n",
        "cm_sklearn = confusion_matrix(test_labels, y_prediction_sklearn)\n",
        "accuracy_sklearn = accuracy_score(test_labels, y_prediction_sklearn)\n",
        "precision_sklearn = precision_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "recall_sklearn = recall_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "f1_sklearn = f1_score(test_labels, y_prediction_sklearn, average=None, zero_division=0)\n",
        "\n",
        "#printing information\n",
        "class_names = [\n",
        "    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
        "    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n",
        "]\n",
        "\n",
        "print(\"\\n===== Decision Tree (Scikit-learn, PCA-50) =====\")\n",
        "print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "print(cm_sklearn)\n",
        "\n",
        "macro_precision = np.mean(precision_sklearn)\n",
        "macro_recall = np.mean(recall_sklearn)\n",
        "macro_f1 = np.mean(f1_sklearn)\n",
        "\n",
        "print(\"\\nOverall:\")\n",
        "print(f\"- Accuracy       : {accuracy_sklearn:.4f}\")\n",
        "print(f\"- Macro Precision: {macro_precision:.4f}\")\n",
        "print(f\"- Macro Recall   : {macro_recall:.4f}\")\n",
        "print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  class {i} ({name}): \"\n",
        "          f\"P={precision_sklearn[i]:.4f} R={recall_sklearn[i]:.4f} F1={f1_sklearn[i]:.4f}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "EqtkSc0cVivz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c357706-193f-40b3-b917-96bfdab83c4a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Decision Tree (Scikit-learn, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[55  5  9  6  2  1  2  0 16  4]\n",
            " [ 3 69  1  4  0  1  3  1  5 13]\n",
            " [ 3  0 42 19  8 10 12  4  2  0]\n",
            " [ 3  0  8 60  1 14  8  4  1  1]\n",
            " [ 3  0  7  8 54  5  3 18  1  1]\n",
            " [ 0  0  6 25  3 58  2  5  1  0]\n",
            " [ 2  0  6 11  5  2 71  2  1  0]\n",
            " [ 1  1  4 13 13 11  0 55  1  1]\n",
            " [18  6  3  0  0  0  2  0 66  5]\n",
            " [ 5  9  0  5  1  0  0  1  7 72]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.6020\n",
            "- Macro Precision: 0.6129\n",
            "- Macro Recall   : 0.6020\n",
            "- Macro F1       : 0.6044\n",
            "  class 0 (airplane): P=0.5914 R=0.5500 F1=0.5699\n",
            "  class 1 (automobile): P=0.7667 R=0.6900 F1=0.7263\n",
            "  class 2 (bird): P=0.4884 R=0.4200 F1=0.4516\n",
            "  class 3 (cat): P=0.3974 R=0.6000 F1=0.4781\n",
            "  class 4 (deer): P=0.6207 R=0.5400 F1=0.5775\n",
            "  class 5 (dog): P=0.5686 R=0.5800 F1=0.5743\n",
            "  class 6 (frog): P=0.6893 R=0.7100 F1=0.6995\n",
            "  class 7 (horse): P=0.6111 R=0.5500 F1=0.5789\n",
            "  class 8 (ship): P=0.6535 R=0.6600 F1=0.6567\n",
            "  class 9 (truck): P=0.7423 R=0.7200 F1=0.7310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "_3eLNmht8HnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_dt_sklearn = metrics_row(test_labels, y_prediction_sklearn, \"Scikit’s implementation of a Decision Tree\")"
      ],
      "metadata": {
        "id": "rmb0qX3e8JiT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5: Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "id": "kFE8yV1wQQTc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "498c1f20"
      },
      "source": [
        "## 5.1: Define the MLP architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d2c72c1"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 512)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.bn_2 = nn.BatchNorm1d(512)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867e0c3d"
      },
      "source": [
        "Define Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b2c3935",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02b284c-7407-48a3-8470-50180bdd4115"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "mlp_initial_model_path = \"mlp_initial.pth\"\n",
        "\n",
        "model = MLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "if os.path.exists(mlp_initial_model_path):\n",
        "    print(\"Loading saved MLP model...\")\n",
        "    model.load_state_dict(torch.load(mlp_initial_model_path, map_location=device))\n",
        "    model.eval() # Set to evaluation mode after loading\n",
        "    print(\"MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Multilayer Perceptron (MLP) created. Model will be trained and saved.\")\n",
        "\n",
        "model.to(device) # Move model to device\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"Training with PyTorch's CrossEntropyLoss\")\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "print(\"Using SGD optimizer with momentum of 0.9\\n\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multilayer Perceptron (MLP) created. Model will be trained and saved.\n",
            "Training with PyTorch's CrossEntropyLoss\n",
            "Using SGD optimizer with momentum of 0.9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate confusion matrix for initial MLP"
      ],
      "metadata": {
        "id": "bjIHvwZSYVr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Ensure mlp_initial_model_path and device are defined if running this cell independently\n",
        "# (they should be defined in the previous cell)\n",
        "mlp_initial_model_path = \"mlp_initial.pth\"\n",
        "\n",
        "# Convert training data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "\n",
        "# Define num_epochs and batch_size\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if not os.path.exists(mlp_initial_model_path):\n",
        "    print(\"Starting training for initial MLP...\")\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Set the model to training mode\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device) # Move data to device\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "            outputs = model(inputs) # Forward pass\n",
        "            loss = criterion(outputs, labels) # Calculate loss\n",
        "            loss.backward() # Backpropagation\n",
        "            optimizer.step() # Update weights\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(model.state_dict(), mlp_initial_model_path)\n",
        "    print(f\"MLP model saved to {mlp_initial_model_path}\")\n",
        "else:\n",
        "    print(\"Skipping training as MLP model already exists and was loaded.\")\n",
        "\n",
        "# Convert test data to PyTorch tensors and move to device\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(test_features_pca_tensor)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "# Convert predictions and labels to numpy array (back to CPU for numpy conversion if on GPU)\n",
        "pred_mlp_initial = predicted.cpu().numpy()\n",
        "\n",
        "# Lines to display the results (Confusion Matrix and metrics)\n",
        "print_eval_report(\n",
        "    \"MLP (Initial, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(), # Pass labels from CPU tensor\n",
        "    pred_mlp_initial,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "GOlX40ikYVRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c17fb40-5244-43b0-c4cc-37e3610fa72d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for initial MLP...\n",
            "Epoch 1/10, Loss: 1.4163\n",
            "Epoch 2/10, Loss: 0.7443\n",
            "Epoch 3/10, Loss: 0.6204\n",
            "Epoch 4/10, Loss: 0.5415\n",
            "Epoch 5/10, Loss: 0.4973\n",
            "Epoch 6/10, Loss: 0.4631\n",
            "Epoch 7/10, Loss: 0.4359\n",
            "Epoch 8/10, Loss: 0.4118\n",
            "Epoch 9/10, Loss: 0.3920\n",
            "Epoch 10/10, Loss: 0.3799\n",
            "Training finished.\n",
            "MLP model saved to mlp_initial.pth\n",
            "\n",
            "===== MLP (Initial, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[76  0  3  2  0  0  1  0 11  7]\n",
            " [ 3 92  0  1  0  0  0  0  0  4]\n",
            " [ 4  0 72  7  3  6  7  0  1  0]\n",
            " [ 0  0  4 75  2 11  6  2  0  0]\n",
            " [ 1  0  5  5 80  2  1  5  1  0]\n",
            " [ 0  0  4 14  2 75  2  2  1  0]\n",
            " [ 1  0  3  3  1  1 90  1  0  0]\n",
            " [ 2  0  0  4  7  4  0 83  0  0]\n",
            " [ 4  0  1  0  0  0  0  0 93  2]\n",
            " [ 3  4  0  2  0  0  0  0  0 91]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8270\n",
            "- Macro Precision: 0.8291\n",
            "- Macro Recall   : 0.8270\n",
            "- Macro F1       : 0.8271\n",
            "  class 0 (airplane): P=0.8085 R=0.7600 F1=0.7835\n",
            "  class 1 (automobile): P=0.9583 R=0.9200 F1=0.9388\n",
            "  class 2 (bird): P=0.7826 R=0.7200 F1=0.7500\n",
            "  class 3 (cat): P=0.6637 R=0.7500 F1=0.7042\n",
            "  class 4 (deer): P=0.8421 R=0.8000 F1=0.8205\n",
            "  class 5 (dog): P=0.7576 R=0.7500 F1=0.7538\n",
            "  class 6 (frog): P=0.8411 R=0.9000 F1=0.8696\n",
            "  class 7 (horse): P=0.8925 R=0.8300 F1=0.8601\n",
            "  class 8 (ship): P=0.8692 R=0.9300 F1=0.8986\n",
            "  class 9 (truck): P=0.8750 R=0.9100 F1=0.8922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "Llh6jVPU85L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_initial = metrics_row(test_labels, pred_mlp_initial, \"MLP\")"
      ],
      "metadata": {
        "id": "woVtd49w86nO"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.1: Adding layers"
      ],
      "metadata": {
        "id": "9E38fAEmUlqx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D20rIrBwUNh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7cd58f5-1fff-4f47-b3fd-bd70243231c5"
      },
      "source": [
        "# Define an MLP with more layers\n",
        "class DeeperMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(DeeperMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 512)\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.bn_2 = nn.BatchNorm1d(512)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(512, 256) # Added a new hidden layer\n",
        "        self.relu_3 = nn.ReLU()\n",
        "        self.layer_4 = nn.Linear(256, num_classes) # Output layer adjusted\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        x = self.relu_3(x)\n",
        "        x = self.layer_4(x)\n",
        "        return x\n",
        "\n",
        "print(\"Deeper Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the deeper MLP model\n",
        "deep_mlp_model_path = \"deeper_mlp.pth\"\n",
        "deeper_model = DeeperMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "deeper_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the deeper model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_deeper = optim.SGD(deeper_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(deep_mlp_model_path):\n",
        "    print(\"Loading saved Deeper MLP model...\")\n",
        "    deeper_model.load_state_dict(torch.load(deep_mlp_model_path, map_location=device))\n",
        "    deeper_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Deeper MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Deeper MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        deeper_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_deeper.zero_grad()\n",
        "            outputs = deeper_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_deeper.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(deeper_model.state_dict(), deep_mlp_model_path)\n",
        "    print(f\"Deeper MLP model saved to {deep_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the deeper model\n",
        "deeper_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = deeper_model(test_features_pca_tensor)\n",
        "    _, predicted_deeper = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the deeper MLP\n",
        "pred_deeper_mlp = predicted_deeper.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Deeper, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_deeper_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deeper Multilayer Perceptron (MLP) created\n",
            "Starting training for Deeper MLP...\n",
            "Epoch 1/10, Loss: 2.0582\n",
            "Epoch 2/10, Loss: 1.3901\n",
            "Epoch 3/10, Loss: 0.9298\n",
            "Epoch 4/10, Loss: 0.7297\n",
            "Epoch 5/10, Loss: 0.6259\n",
            "Epoch 6/10, Loss: 0.5655\n",
            "Epoch 7/10, Loss: 0.5129\n",
            "Epoch 8/10, Loss: 0.4775\n",
            "Epoch 9/10, Loss: 0.4482\n",
            "Epoch 10/10, Loss: 0.4202\n",
            "Training finished.\n",
            "Deeper MLP model saved to deeper_mlp.pth\n",
            "\n",
            "===== MLP (Deeper, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[84  1  1  1  0  0  1  0  8  4]\n",
            " [ 3 92  0  1  0  0  0  0  1  3]\n",
            " [ 6  0 75  6  3  5  5  0  0  0]\n",
            " [ 1  0  4 75  1 13  5  1  0  0]\n",
            " [ 2  0  3  5 78  3  0  7  2  0]\n",
            " [ 0  0  4 14  1 76  2  2  1  0]\n",
            " [ 1  0  4  5  2  4 83  1  0  0]\n",
            " [ 2  1  0  4  9  4  0 80  0  0]\n",
            " [ 3  0  1  0  0  0  0  0 94  2]\n",
            " [ 2  3  0  1  0  0  0  0  1 93]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8300\n",
            "- Macro Precision: 0.8329\n",
            "- Macro Recall   : 0.8300\n",
            "- Macro F1       : 0.8306\n",
            "  class 0 (airplane): P=0.8077 R=0.8400 F1=0.8235\n",
            "  class 1 (automobile): P=0.9485 R=0.9200 F1=0.9340\n",
            "  class 2 (bird): P=0.8152 R=0.7500 F1=0.7812\n",
            "  class 3 (cat): P=0.6696 R=0.7500 F1=0.7075\n",
            "  class 4 (deer): P=0.8298 R=0.7800 F1=0.8041\n",
            "  class 5 (dog): P=0.7238 R=0.7600 F1=0.7415\n",
            "  class 6 (frog): P=0.8646 R=0.8300 F1=0.8469\n",
            "  class 7 (horse): P=0.8791 R=0.8000 F1=0.8377\n",
            "  class 8 (ship): P=0.8785 R=0.9400 F1=0.9082\n",
            "  class 9 (truck): P=0.9118 R=0.9300 F1=0.9208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "YOsIBx4R9GWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_deeper = metrics_row(test_labels, pred_deeper_mlp, \"MLP (Deeper, PCA-50)\")"
      ],
      "metadata": {
        "id": "tV5maTPf9H_y"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2.2: Removing layers"
      ],
      "metadata": {
        "id": "iffaHBamU2N8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with fewer layers (e.g. one hidden layer)\n",
        "class ShallowerMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ShallowerMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 256) # Reduced hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(256, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        return x\n",
        "\n",
        "print(\"Shallower Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the shallower MLP model\n",
        "shallower_mlp_model_path = \"shallower_mlp.pth\"\n",
        "shallower_model = ShallowerMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "shallower_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the shallower model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_shallower = optim.SGD(shallower_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(shallower_mlp_model_path):\n",
        "    print(\"Loading saved Shallower MLP model...\")\n",
        "    shallower_model.load_state_dict(torch.load(shallower_mlp_model_path, map_location=device))\n",
        "    shallower_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Shallower MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Shallower MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        shallower_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_shallower.zero_grad()\n",
        "            outputs = shallower_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_shallower.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(shallower_model.state_dict(), shallower_mlp_model_path)\n",
        "    print(f\"Shallower MLP model saved to {shallower_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the shallower model\n",
        "shallower_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = shallower_model(test_features_pca_tensor)\n",
        "    _, predicted_shallower = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the shallower MLP\n",
        "pred_shallower_mlp = predicted_shallower.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Shallower, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_shallower_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "jOjmSVAdkjr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c73bafaa-7fec-417d-c2eb-0a3d7550cac7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shallower Multilayer Perceptron (MLP) created\n",
            "Starting training for Shallower MLP...\n",
            "Epoch 1/10, Loss: 1.7531\n",
            "Epoch 2/10, Loss: 1.0376\n",
            "Epoch 3/10, Loss: 0.8103\n",
            "Epoch 4/10, Loss: 0.7070\n",
            "Epoch 5/10, Loss: 0.6437\n",
            "Epoch 6/10, Loss: 0.5933\n",
            "Epoch 7/10, Loss: 0.5646\n",
            "Epoch 8/10, Loss: 0.5394\n",
            "Epoch 9/10, Loss: 0.5291\n",
            "Epoch 10/10, Loss: 0.5022\n",
            "Training finished.\n",
            "Shallower MLP model saved to shallower_mlp.pth\n",
            "\n",
            "===== MLP (Shallower, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[81  0  2  1  0  0  0  0 11  5]\n",
            " [ 2 94  0  1  0  0  0  0  0  3]\n",
            " [ 8  0 70  3  3  6  8  1  1  0]\n",
            " [ 1  0  3 70  2 15  8  1  0  0]\n",
            " [ 2  0  3  5 77  4  0  6  1  2]\n",
            " [ 0  0  4 12  2 76  2  3  1  0]\n",
            " [ 1  0  2  2  4  2 88  0  1  0]\n",
            " [ 1  1  1  4  7  6  0 77  1  2]\n",
            " [ 7  0  2  0  0  0  0  0 88  3]\n",
            " [ 1  5  0  1  0  0  0  0  1 92]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8130\n",
            "- Macro Precision: 0.8141\n",
            "- Macro Recall   : 0.8130\n",
            "- Macro F1       : 0.8124\n",
            "  class 0 (airplane): P=0.7788 R=0.8100 F1=0.7941\n",
            "  class 1 (automobile): P=0.9400 R=0.9400 F1=0.9400\n",
            "  class 2 (bird): P=0.8046 R=0.7000 F1=0.7487\n",
            "  class 3 (cat): P=0.7071 R=0.7000 F1=0.7035\n",
            "  class 4 (deer): P=0.8105 R=0.7700 F1=0.7897\n",
            "  class 5 (dog): P=0.6972 R=0.7600 F1=0.7273\n",
            "  class 6 (frog): P=0.8302 R=0.8800 F1=0.8544\n",
            "  class 7 (horse): P=0.8750 R=0.7700 F1=0.8191\n",
            "  class 8 (ship): P=0.8381 R=0.8800 F1=0.8585\n",
            "  class 9 (truck): P=0.8598 R=0.9200 F1=0.8889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "B9richDg9Uk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_shallower = metrics_row(test_labels, pred_shallower_mlp, \"MLP (Shallower, PCA-50)\")"
      ],
      "metadata": {
        "id": "41UXRO6B9Wm2"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.1: Larger Hidden Layer"
      ],
      "metadata": {
        "id": "NsYWt-gPVEhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with larger hidden layers (e.g. 1024 units)\n",
        "class WiderMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(WiderMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 1024) # Increased hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(1024, 1024) # Increased hidden layer size\n",
        "        self.bn_2 = nn.BatchNorm1d(1024)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(1024, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "print(\"Wider Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the wider MLP model\n",
        "wider_mlp_model_path = \"wider_mlp.pth\"\n",
        "wider_model = WiderMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "wider_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the wider model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_wider = optim.SGD(wider_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(wider_mlp_model_path):\n",
        "    print(\"Loading saved Wider MLP model...\")\n",
        "    wider_model.load_state_dict(torch.load(wider_mlp_model_path, map_location=device))\n",
        "    wider_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Wider MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Wider MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        wider_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_wider.zero_grad()\n",
        "            outputs = wider_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_wider.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(wider_model.state_dict(), wider_mlp_model_path)\n",
        "    print(f\"Wider MLP model saved to {wider_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the wider model\n",
        "wider_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = wider_model(test_features_pca_tensor)\n",
        "    _, predicted_wider = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the wider MLP\n",
        "pred_wider_mlp = predicted_wider.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Wider, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_wider_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "0sKYJiLqlM0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66805ea5-5453-4cd4-d629-89f79f95fdec"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wider Multilayer Perceptron (MLP) created\n",
            "Starting training for Wider MLP...\n",
            "Epoch 1/10, Loss: 1.1422\n",
            "Epoch 2/10, Loss: 0.5851\n",
            "Epoch 3/10, Loss: 0.4985\n",
            "Epoch 4/10, Loss: 0.4482\n",
            "Epoch 5/10, Loss: 0.4055\n",
            "Epoch 6/10, Loss: 0.3967\n",
            "Epoch 7/10, Loss: 0.3591\n",
            "Epoch 8/10, Loss: 0.3377\n",
            "Epoch 9/10, Loss: 0.3187\n",
            "Epoch 10/10, Loss: 0.3096\n",
            "Training finished.\n",
            "Wider MLP model saved to wider_mlp.pth\n",
            "\n",
            "===== MLP (Wider, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[88  0  1  1  0  0  0  0  7  3]\n",
            " [ 3 91  0  1  0  0  0  0  0  5]\n",
            " [ 6  0 71  8  5  4  6  0  0  0]\n",
            " [ 1  0  2 74  2 13  7  1  0  0]\n",
            " [ 2  0  3  7 74  2  3  8  1  0]\n",
            " [ 0  0  5 15  2 72  2  3  1  0]\n",
            " [ 1  0  3  4  2  1 88  0  1  0]\n",
            " [ 0  0  0  5  7  3  0 85  0  0]\n",
            " [ 4  0  1  0  0  0  0  0 93  2]\n",
            " [ 3  4  0  2  0  0  0  0  0 91]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8270\n",
            "- Macro Precision: 0.8303\n",
            "- Macro Recall   : 0.8270\n",
            "- Macro F1       : 0.8273\n",
            "  class 0 (airplane): P=0.8148 R=0.8800 F1=0.8462\n",
            "  class 1 (automobile): P=0.9579 R=0.9100 F1=0.9333\n",
            "  class 2 (bird): P=0.8256 R=0.7100 F1=0.7634\n",
            "  class 3 (cat): P=0.6325 R=0.7400 F1=0.6820\n",
            "  class 4 (deer): P=0.8043 R=0.7400 F1=0.7708\n",
            "  class 5 (dog): P=0.7579 R=0.7200 F1=0.7385\n",
            "  class 6 (frog): P=0.8302 R=0.8800 F1=0.8544\n",
            "  class 7 (horse): P=0.8763 R=0.8500 F1=0.8629\n",
            "  class 8 (ship): P=0.9029 R=0.9300 F1=0.9163\n",
            "  class 9 (truck): P=0.9010 R=0.9100 F1=0.9055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metric"
      ],
      "metadata": {
        "id": "FXoC9J9N9eTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_wider = metrics_row(test_labels, pred_wider_mlp, \"MLP (Wider, PCA-50)\")"
      ],
      "metadata": {
        "id": "BlCgegvK9fvf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3.2: Smaller Hidden Layer"
      ],
      "metadata": {
        "id": "hghMs375VHO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an MLP with smaller hidden layers (e.g. 128 units)\n",
        "class NarrowerMLP(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NarrowerMLP, self).__init__()\n",
        "        self.layer_1 = nn.Linear(input_size, 128) # Decreased hidden layer size\n",
        "        self.relu_1 = nn.ReLU()\n",
        "        self.layer_2 = nn.Linear(128, 128) # Decreased hidden layer size\n",
        "        self.bn_2 = nn.BatchNorm1d(128)\n",
        "        self.relu_2 = nn.ReLU()\n",
        "        self.layer_3 = nn.Linear(128, num_classes) # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.relu_1(x)\n",
        "        x = self.layer_2(x)\n",
        "        x = self.bn_2(x)\n",
        "        x = self.relu_2(x)\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "print(\"Narrower Multilayer Perceptron (MLP) created\")\n",
        "\n",
        "# Instantiate the narrower MLP model\n",
        "narrower_mlp_model_path = \"narrower_mlp.pth\"\n",
        "narrower_model = NarrowerMLP(input_size=train_features_pca.shape[1], num_classes=10)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "narrower_model.to(device)\n",
        "\n",
        "# Define Loss Function and Optimizer for the narrower model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_narrower = optim.SGD(narrower_model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "train_features_pca_tensor = torch.tensor(train_features_pca, dtype=torch.float32).to(device)\n",
        "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long).to(device)\n",
        "test_features_pca_tensor = torch.tensor(test_features_pca, dtype=torch.float32).to(device)\n",
        "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long).to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(train_features_pca_tensor, train_labels_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "if os.path.exists(narrower_mlp_model_path):\n",
        "    print(\"Loading saved Narrower MLP model...\")\n",
        "    narrower_model.load_state_dict(torch.load(narrower_mlp_model_path, map_location=device))\n",
        "    narrower_model.eval() # Set to evaluation mode after loading\n",
        "    print(\"Narrower MLP model loaded successfully.\")\n",
        "else:\n",
        "    print(\"Starting training for Narrower MLP...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        narrower_model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer_narrower.zero_grad()\n",
        "            outputs = narrower_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_narrower.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    torch.save(narrower_model.state_dict(), narrower_mlp_model_path)\n",
        "    print(f\"Narrower MLP model saved to {narrower_mlp_model_path}\")\n",
        "\n",
        "# Evaluate the narrower model\n",
        "narrower_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = narrower_model(test_features_pca_tensor)\n",
        "    _, predicted_narrower = torch.max(outputs.data, 1)\n",
        "\n",
        "# Generate and print evaluation report for the narrower MLP\n",
        "pred_narrower_mlp = predicted_narrower.cpu().numpy()\n",
        "print_eval_report(\n",
        "    \"MLP (Narrower, PCA-50)\",\n",
        "    test_labels_tensor.cpu().numpy(),\n",
        "    pred_narrower_mlp,\n",
        "    class_names=cifar10_classes\n",
        ")"
      ],
      "metadata": {
        "id": "HevvNWFGm0SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219622e8-c417-4377-e237-667ae470bd2e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Narrower Multilayer Perceptron (MLP) created\n",
            "Starting training for Narrower MLP...\n",
            "Epoch 1/10, Loss: 1.9326\n",
            "Epoch 2/10, Loss: 1.3068\n",
            "Epoch 3/10, Loss: 1.0099\n",
            "Epoch 4/10, Loss: 0.8434\n",
            "Epoch 5/10, Loss: 0.7374\n",
            "Epoch 6/10, Loss: 0.6706\n",
            "Epoch 7/10, Loss: 0.6258\n",
            "Epoch 8/10, Loss: 0.5840\n",
            "Epoch 9/10, Loss: 0.5515\n",
            "Epoch 10/10, Loss: 0.5191\n",
            "Training finished.\n",
            "Narrower MLP model saved to narrower_mlp.pth\n",
            "\n",
            "===== MLP (Narrower, PCA-50) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[84  1  1  1  0  0  0  0 10  3]\n",
            " [ 3 90  0  1  0  0  0  0  2  4]\n",
            " [ 7  0 66  4  4  8  9  2  0  0]\n",
            " [ 0  0  5 69  2 13  9  1  1  0]\n",
            " [ 3  0  4  6 71  3  1 10  2  0]\n",
            " [ 0  0  4 13  1 77  2  2  1  0]\n",
            " [ 2  0  1  4  4  1 87  0  1  0]\n",
            " [ 1  0  0  3  7  6  0 82  0  1]\n",
            " [ 5  0  1  0  0  0  0  0 92  2]\n",
            " [ 2  4  0  0  0  0  0  0  2 92]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.8100\n",
            "- Macro Precision: 0.8113\n",
            "- Macro Recall   : 0.8100\n",
            "- Macro F1       : 0.8090\n",
            "  class 0 (airplane): P=0.7850 R=0.8400 F1=0.8116\n",
            "  class 1 (automobile): P=0.9474 R=0.9000 F1=0.9231\n",
            "  class 2 (bird): P=0.8049 R=0.6600 F1=0.7253\n",
            "  class 3 (cat): P=0.6832 R=0.6900 F1=0.6866\n",
            "  class 4 (deer): P=0.7978 R=0.7100 F1=0.7513\n",
            "  class 5 (dog): P=0.7130 R=0.7700 F1=0.7404\n",
            "  class 6 (frog): P=0.8056 R=0.8700 F1=0.8365\n",
            "  class 7 (horse): P=0.8454 R=0.8200 F1=0.8325\n",
            "  class 8 (ship): P=0.8288 R=0.9200 F1=0.8720\n",
            "  class 9 (truck): P=0.9020 R=0.9200 F1=0.9109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting metrics"
      ],
      "metadata": {
        "id": "5Q42l-8y9wgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_mlp_narrower = metrics_row(test_labels, pred_narrower_mlp, \"MLP (Narrower, PCA-50)\")"
      ],
      "metadata": {
        "id": "5vGfHsky9x0d"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6: Convolutional Neural Network (CNN)"
      ],
      "metadata": {
        "id": "f6cpBCyO-EHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1: Implementing and training a VGG11 net"
      ],
      "metadata": {
        "id": "xZhvFqjJ-z-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `torch.nn.CrossEntropyLoss`, and optimize using SGD optimizer with `momentum=0.9`"
      ],
      "metadata": {
        "id": "4Y0LqrKg-9Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing VGG11 - according to assignment description"
      ],
      "metadata": {
        "id": "9r5tQFjCpTTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11, self).__init__()\n",
        "    #extracting image features\n",
        "    self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "axdbMIY2pW7o"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing Data for CNN (resizing to 32x32)"
      ],
      "metadata": {
        "id": "JE82L2Gm1ngj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "39lqp42S1wtd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the CNN"
      ],
      "metadata": {
        "id": "p4spd-igBW1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11(num_classes=10).to(device)\n",
        "\n",
        "#where to store the model\n",
        "model_path = \"vgg11_base.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vgg11_base.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "#Train from scratch\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeTNSVGYBY5z",
        "outputId": "b62404e5-7136-4353-c637-e197ac91b9c7",
        "collapsed": true
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.1424\n",
            "Epoch [2/10] - Loss: 1.7154\n",
            "Epoch [3/10] - Loss: 1.4265\n",
            "Epoch [4/10] - Loss: 1.1932\n",
            "Epoch [5/10] - Loss: 0.9414\n",
            "Epoch [6/10] - Loss: 0.6971\n",
            "Epoch [7/10] - Loss: 0.4817\n",
            "Epoch [8/10] - Loss: 0.3826\n",
            "Epoch [9/10] - Loss: 0.2400\n",
            "Epoch [10/10] - Loss: 0.1239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of CNN"
      ],
      "metadata": {
        "id": "U369LpsB61yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn, preds_cnn = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "#collecting metric\n",
        "row_cnn = metrics_row(labels_cnn, preds_cnn, \"CNN (VGG11)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl9y5DMe63oj",
        "outputId": "a20ab4a5-e4e0-497e-bfa7-12acb017402a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[51  4 10  5  3  1  8  1 15  2]\n",
            " [ 3 82  0  2  0  0  2  0  5  6]\n",
            " [ 9  0 48 13  6  9 10  2  2  1]\n",
            " [ 1  1 12 48  2 23  8  3  0  2]\n",
            " [ 4  2 15 14 33 10  6 12  3  1]\n",
            " [ 1  0 10 24  1 52  2  7  2  1]\n",
            " [ 0  0  7 18  2  3 69  1  0  0]\n",
            " [ 3  0  0 10  5 11  1 64  2  4]\n",
            " [ 7 10  1  2  2  1  5  0 68  4]\n",
            " [ 3 20  1  4  0  0  4  0 13 55]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5700\n",
            "- Macro Precision: 0.5852\n",
            "- Macro Recall   : 0.5700\n",
            "- Macro F1       : 0.5692\n",
            "  class 0 (airplane): P=0.6220 R=0.5100 F1=0.5604\n",
            "  class 1 (automobile): P=0.6891 R=0.8200 F1=0.7489\n",
            "  class 2 (bird): P=0.4615 R=0.4800 F1=0.4706\n",
            "  class 3 (cat): P=0.3429 R=0.4800 F1=0.4000\n",
            "  class 4 (deer): P=0.6111 R=0.3300 F1=0.4286\n",
            "  class 5 (dog): P=0.4727 R=0.5200 F1=0.4952\n",
            "  class 6 (frog): P=0.6000 R=0.6900 F1=0.6419\n",
            "  class 7 (horse): P=0.7111 R=0.6400 F1=0.6737\n",
            "  class 8 (ship): P=0.6182 R=0.6800 F1=0.6476\n",
            "  class 9 (truck): P=0.7237 R=0.5500 F1=0.6250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2: Adding convolutional layers"
      ],
      "metadata": {
        "id": "yjibcjsf_XJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_Add(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            # added an extra layer\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # final feature map = 512×2×2 = 2048\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 2 * 2, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "l_N0Dr5fNP8D"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data after adding a layer"
      ],
      "metadata": {
        "id": "0Iihcw_pxSRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "E7pEnykOxYAg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN after adding layer"
      ],
      "metadata": {
        "id": "WiaPwFU42Ia8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_Add(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_add.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_add.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIlcsIX72LB4",
        "outputId": "cab312f0-2e87-422a-ea2e-63ae2c60e25c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.1776\n",
            "Epoch [2/10] - Loss: 1.7579\n",
            "Epoch [3/10] - Loss: 1.4853\n",
            "Epoch [4/10] - Loss: 1.2147\n",
            "Epoch [5/10] - Loss: 0.9445\n",
            "Epoch [6/10] - Loss: 0.6897\n",
            "Epoch [7/10] - Loss: 0.6461\n",
            "Epoch [8/10] - Loss: 0.4287\n",
            "Epoch [9/10] - Loss: 0.3466\n",
            "Epoch [10/10] - Loss: 0.1936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation CNN after adding layer"
      ],
      "metadata": {
        "id": "TWF_eRsIAbPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11-Added, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_add, preds_cnn_add = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_add    = metrics_row(labels_cnn_add, preds_cnn_add, \"CNN (add)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR1ssLzQAdrL",
        "outputId": "d2b7d40a-db03-4f9e-99a7-dcef307bde36"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11-Added, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[53  3  2  4  4  5  3  6 16  4]\n",
            " [ 3 68  0  5  1  4  2  3  5  9]\n",
            " [11  1 35 17  6 16  7  5  2  0]\n",
            " [ 2  1  3 39  1 24 14 13  0  3]\n",
            " [ 2  1 12 17 33  9  7 18  1  0]\n",
            " [ 0  0  4 23  3 56  2 12  0  0]\n",
            " [ 0  0  0 16  6  3 74  1  0  0]\n",
            " [ 0  0  0  8  4  9  0 75  0  4]\n",
            " [10  8  1  5  1  0  0  0 71  4]\n",
            " [ 2 11  1  9  0  0  5  6  9 57]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5610\n",
            "- Macro Precision: 0.5825\n",
            "- Macro Recall   : 0.5610\n",
            "- Macro F1       : 0.5604\n",
            "  class 0 (airplane): P=0.6386 R=0.5300 F1=0.5792\n",
            "  class 1 (automobile): P=0.7312 R=0.6800 F1=0.7047\n",
            "  class 2 (bird): P=0.6034 R=0.3500 F1=0.4430\n",
            "  class 3 (cat): P=0.2727 R=0.3900 F1=0.3210\n",
            "  class 4 (deer): P=0.5593 R=0.3300 F1=0.4151\n",
            "  class 5 (dog): P=0.4444 R=0.5600 F1=0.4956\n",
            "  class 6 (frog): P=0.6491 R=0.7400 F1=0.6916\n",
            "  class 7 (horse): P=0.5396 R=0.7500 F1=0.6276\n",
            "  class 8 (ship): P=0.6827 R=0.7100 F1=0.6961\n",
            "  class 9 (truck): P=0.7037 R=0.5700 F1=0.6298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3: Removing convolutional layers"
      ],
      "metadata": {
        "id": "itWicRDo_dfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_Remove(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11_Remove, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "      nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "      nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "      #removed 3 layers\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Linear(512*4*4, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "      nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "      nn.Linear(4096, num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    return self.classifier(x)"
      ],
      "metadata": {
        "id": "6hptIWaHOepb"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data after removing layer"
      ],
      "metadata": {
        "id": "CZO8Nrd5GVpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "hbJJtX0kGZHn"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN after removing layer"
      ],
      "metadata": {
        "id": "VHnI1VKCHaeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_Remove(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_remove.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_remove.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn6AbKMxI4Nu",
        "outputId": "358dc377-24c3-413b-d67b-aa54eaf1943f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.1993\n",
            "Epoch [2/10] - Loss: 1.8422\n",
            "Epoch [3/10] - Loss: 1.5895\n",
            "Epoch [4/10] - Loss: 1.4464\n",
            "Epoch [5/10] - Loss: 1.3135\n",
            "Epoch [6/10] - Loss: 1.1836\n",
            "Epoch [7/10] - Loss: 1.0615\n",
            "Epoch [8/10] - Loss: 0.9439\n",
            "Epoch [9/10] - Loss: 0.8021\n",
            "Epoch [10/10] - Loss: 0.6571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating CNN after removing layer"
      ],
      "metadata": {
        "id": "deScbokpHsfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11-Remove, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_remove, preds_cnn_remove = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_remove = metrics_row(labels_cnn_remove, preds_cnn_remove, \"CNN (remove)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5ot0a8JHvQf",
        "outputId": "3dc82e9e-9725-4c7b-9e5d-24f6afd01fef"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11-Remove, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[49  7  5  2  1  0  4  5 13 14]\n",
            " [ 1 80  0  1  1  0  0  1  0 16]\n",
            " [ 9  1 33  5 25  6  5  9  3  4]\n",
            " [ 2  3  6 26 12 12 14 17  2  6]\n",
            " [ 3  2  7  2 52  4  3 24  1  2]\n",
            " [ 0  1  6 17  4 41  2 25  1  3]\n",
            " [ 0  2  1  4 15  1 66  3  0  8]\n",
            " [ 2  1  1  4  1  3  0 87  0  1]\n",
            " [ 8  9  1  1  1  0  0  1 66 13]\n",
            " [ 2  6  1  1  0  0  0  4  4 82]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5820\n",
            "- Macro Precision: 0.5869\n",
            "- Macro Recall   : 0.5820\n",
            "- Macro F1       : 0.5686\n",
            "  class 0 (airplane): P=0.6447 R=0.4900 F1=0.5568\n",
            "  class 1 (automobile): P=0.7143 R=0.8000 F1=0.7547\n",
            "  class 2 (bird): P=0.5410 R=0.3300 F1=0.4099\n",
            "  class 3 (cat): P=0.4127 R=0.2600 F1=0.3190\n",
            "  class 4 (deer): P=0.4643 R=0.5200 F1=0.4906\n",
            "  class 5 (dog): P=0.6119 R=0.4100 F1=0.4910\n",
            "  class 6 (frog): P=0.7021 R=0.6600 F1=0.6804\n",
            "  class 7 (horse): P=0.4943 R=0.8700 F1=0.6304\n",
            "  class 8 (ship): P=0.7333 R=0.6600 F1=0.6947\n",
            "  class 9 (truck): P=0.5503 R=0.8200 F1=0.6586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4: Larger kernel size"
      ],
      "metadata": {
        "id": "U-FpmLSs_hEo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kernel size of 5x5"
      ],
      "metadata": {
        "id": "JTIT3RiJXC_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VGG11_kernel5(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(VGG11_kernel5, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "            #Chaning the kernel size to 5x5\n",
        "            nn.Conv2d(3, 64, 5, 1, 2), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 5, 1, 2), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 5, 1, 2), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 5, 1, 2), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(256, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, 5, 1, 2), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "    self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "06HdLIBcXM56"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare data for kernel 5x5"
      ],
      "metadata": {
        "id": "pf2rI0axX9y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_cnn = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "trainset_cnn = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_cnn)\n",
        "testset_cnn = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_cnn)\n",
        "\n",
        "trainset_cnn = get_subset(trainset_cnn, 500)\n",
        "testset_cnn = get_subset(testset_cnn, 100)\n",
        "\n",
        "trainloader = DataLoader(trainset_cnn, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset_cnn, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "aLEN59XBYAnR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training CNN with 5x5 kernel size"
      ],
      "metadata": {
        "id": "mI6dR3F3YoOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VGG11_kernel5(num_classes=10).to(device)\n",
        "\n",
        "model_path = \"vgg11_k5.pth\"\n",
        "\n",
        "#checking if model was saved and load it\n",
        "if os.path.exists('vvgg11_k5.pth'):\n",
        "  print(\"Loading saved model...\")\n",
        "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "  model.eval()\n",
        "else:\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      running_loss = 0.0\n",
        "      for images, labels in trainloader:\n",
        "          images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          running_loss += loss.item()\n",
        "\n",
        "      #printing the loss to see that the model is training\n",
        "      print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss/len(trainloader):.4f}\")\n",
        "  torch.save(model.state_dict(), model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NG4Ibsp_Ythe",
        "outputId": "62a1362b-c39b-4650-8fa2-a9b88e69d668"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Loss: 2.0477\n",
            "Epoch [2/10] - Loss: 1.6028\n",
            "Epoch [3/10] - Loss: 1.3285\n",
            "Epoch [4/10] - Loss: 1.1040\n",
            "Epoch [5/10] - Loss: 0.8558\n",
            "Epoch [6/10] - Loss: 0.6944\n",
            "Epoch [7/10] - Loss: 0.6220\n",
            "Epoch [8/10] - Loss: 0.4390\n",
            "Epoch [9/10] - Loss: 0.2942\n",
            "Epoch [10/10] - Loss: 0.4437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating the CNN with kernel size 5x5"
      ],
      "metadata": {
        "id": "JjjYYtAkYyBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, testloader, class_names):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in testloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
        "    macro_prec, macro_rec, macro_f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
        "\n",
        "    print(\"===== CNN (VGG11_Kernel5, CIFAR-10) =====\")\n",
        "    print(\"Confusion Matrix (rows=true, cols=pred):\")\n",
        "    print(cm)\n",
        "    print(\"\\nOverall:\")\n",
        "    print(f\"- Accuracy       : {acc:.4f}\")\n",
        "    print(f\"- Macro Precision: {macro_prec:.4f}\")\n",
        "    print(f\"- Macro Recall   : {macro_rec:.4f}\")\n",
        "    print(f\"- Macro F1       : {macro_f1:.4f}\")\n",
        "\n",
        "    for i, name in enumerate(class_names):\n",
        "        print(f\"  class {i} ({name}): P={prec[i]:.4f} R={rec[i]:.4f} F1={f1[i]:.4f}\")\n",
        "    return all_labels, all_preds\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "labels_cnn_k5, preds_cnn_k5 = evaluate_model(model, testloader, class_names)\n",
        "\n",
        "row_cnn_k5      = metrics_row(labels_cnn_k5, preds_cnn_k5, \"CNN (Kernel 5x5)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdmkXbhIY0my",
        "outputId": "8e9db70c-9a0d-4515-f6a9-6aecf382446c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== CNN (VGG11_Kernel5, CIFAR-10) =====\n",
            "Confusion Matrix (rows=true, cols=pred):\n",
            "[[38 22  1  1  2  1  4  1 24  6]\n",
            " [ 0 97  0  0  0  0  0  0  2  1]\n",
            " [17  6 25  7 19 10  7  8  1  0]\n",
            " [ 3 17  5 19 14 15 20  4  0  3]\n",
            " [ 4  9  5  3 53  4  7  9  1  5]\n",
            " [ 0  6  8 11  8 44 10  9  2  2]\n",
            " [ 0 13  0  0 21  2 62  0  1  1]\n",
            " [ 2  8  1  1 11  6  2 62  1  6]\n",
            " [ 4 27  0  0  0  0  0  0 63  6]\n",
            " [ 1 51  0  0  0  0  0  1  5 42]]\n",
            "\n",
            "Overall:\n",
            "- Accuracy       : 0.5050\n",
            "- Macro Precision: 0.5315\n",
            "- Macro Recall   : 0.5050\n",
            "- Macro F1       : 0.4898\n",
            "  class 0 (airplane): P=0.5507 R=0.3800 F1=0.4497\n",
            "  class 1 (automobile): P=0.3789 R=0.9700 F1=0.5449\n",
            "  class 2 (bird): P=0.5556 R=0.2500 F1=0.3448\n",
            "  class 3 (cat): P=0.4524 R=0.1900 F1=0.2676\n",
            "  class 4 (deer): P=0.4141 R=0.5300 F1=0.4649\n",
            "  class 5 (dog): P=0.5366 R=0.4400 F1=0.4835\n",
            "  class 6 (frog): P=0.5536 R=0.6200 F1=0.5849\n",
            "  class 7 (horse): P=0.6596 R=0.6200 F1=0.6392\n",
            "  class 8 (ship): P=0.6300 R=0.6300 F1=0.6300\n",
            "  class 9 (truck): P=0.5833 R=0.4200 F1=0.4884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5: Smaller kernel size"
      ],
      "metadata": {
        "id": "m2QfRA_5_qNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kept the kernel size 3x3 as small, which was already implemented above"
      ],
      "metadata": {
        "id": "BwqvygAWdyF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7: Evaluation Table"
      ],
      "metadata": {
        "id": "0W_9adGYnrv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Evaluation Table"
      ],
      "metadata": {
        "id": "IuflfyWmnvLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = [\n",
        "    row_gnb_scratch,\n",
        "    row_gnb_sklearn,\n",
        "    row_dt_scratch,\n",
        "    row_dt_sklearn,\n",
        "    row_mlp_initial,\n",
        "    row_mlp_deeper,\n",
        "    row_mlp_shallower,\n",
        "    row_mlp_wider,\n",
        "    row_mlp_narrower,\n",
        "    row_cnn,\n",
        "    row_cnn_add,\n",
        "    row_cnn_remove,\n",
        "    row_cnn_k5\n",
        "]\n",
        "\n",
        "df_results = pd.DataFrame(rows)\n",
        "df_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "SBvwv-_WoLQu",
        "outputId": "d8c70769-1b4e-4826-bdee-0407b79c427a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         Model  Accuracy  Macro Precision  \\\n",
              "0                        Naive Bayes (Scratch)     0.790         0.795115   \n",
              "1                Scikit’s Gaussian Naive Bayes     0.790         0.795115   \n",
              "2                      Decision Tree (Scratch)     0.599         0.609647   \n",
              "3   Scikit’s implementation of a Decision Tree     0.602         0.612927   \n",
              "4                                          MLP     0.827         0.829060   \n",
              "5                         MLP (Deeper, PCA-50)     0.830         0.832858   \n",
              "6                      MLP (Shallower, PCA-50)     0.813         0.814139   \n",
              "7                          MLP (Wider, PCA-50)     0.827         0.830339   \n",
              "8                       MLP (Narrower, PCA-50)     0.810         0.811288   \n",
              "9                                  CNN (VGG11)     0.570         0.585224   \n",
              "10                                   CNN (add)     0.561         0.582477   \n",
              "11                                CNN (remove)     0.582         0.586905   \n",
              "12                            CNN (Kernel 5x5)     0.505         0.531469   \n",
              "\n",
              "    Macro Recall  Macro F1  \n",
              "0          0.790  0.790580  \n",
              "1          0.790  0.790580  \n",
              "2          0.599  0.601414  \n",
              "3          0.602  0.604390  \n",
              "4          0.827  0.827116  \n",
              "5          0.830  0.830556  \n",
              "6          0.813  0.812426  \n",
              "7          0.827  0.827329  \n",
              "8          0.810  0.809018  \n",
              "9          0.570  0.569186  \n",
              "10         0.561  0.560371  \n",
              "11         0.582  0.568629  \n",
              "12         0.505  0.489796  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-21ac0c52-9e6a-40bc-ab1c-6e5ba8160ea6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Macro Precision</th>\n",
              "      <th>Macro Recall</th>\n",
              "      <th>Macro F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Naive Bayes (Scratch)</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.795115</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.790580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Scikit’s Gaussian Naive Bayes</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.795115</td>\n",
              "      <td>0.790</td>\n",
              "      <td>0.790580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision Tree (Scratch)</td>\n",
              "      <td>0.599</td>\n",
              "      <td>0.609647</td>\n",
              "      <td>0.599</td>\n",
              "      <td>0.601414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Scikit’s implementation of a Decision Tree</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.612927</td>\n",
              "      <td>0.602</td>\n",
              "      <td>0.604390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MLP</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.829060</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.827116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MLP (Deeper, PCA-50)</td>\n",
              "      <td>0.830</td>\n",
              "      <td>0.832858</td>\n",
              "      <td>0.830</td>\n",
              "      <td>0.830556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>MLP (Shallower, PCA-50)</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.814139</td>\n",
              "      <td>0.813</td>\n",
              "      <td>0.812426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>MLP (Wider, PCA-50)</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.830339</td>\n",
              "      <td>0.827</td>\n",
              "      <td>0.827329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MLP (Narrower, PCA-50)</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.811288</td>\n",
              "      <td>0.810</td>\n",
              "      <td>0.809018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>CNN (VGG11)</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.585224</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.569186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>CNN (add)</td>\n",
              "      <td>0.561</td>\n",
              "      <td>0.582477</td>\n",
              "      <td>0.561</td>\n",
              "      <td>0.560371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>CNN (remove)</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.586905</td>\n",
              "      <td>0.582</td>\n",
              "      <td>0.568629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>CNN (Kernel 5x5)</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.531469</td>\n",
              "      <td>0.505</td>\n",
              "      <td>0.489796</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21ac0c52-9e6a-40bc-ab1c-6e5ba8160ea6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-21ac0c52-9e6a-40bc-ab1c-6e5ba8160ea6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-21ac0c52-9e6a-40bc-ab1c-6e5ba8160ea6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-295d6348-10d2-4501-bbc5-77e47ff220f1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-295d6348-10d2-4501-bbc5-77e47ff220f1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-295d6348-10d2-4501-bbc5-77e47ff220f1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_266ca9ae-0c37-40ed-b03e-f911bfb69a83\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_266ca9ae-0c37-40ed-b03e-f911bfb69a83 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"CNN (remove)\",\n          \"CNN (VGG11)\",\n          \"Naive Bayes (Scratch)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12851693493116992,\n        \"min\": 0.505,\n        \"max\": 0.83,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.813,\n          0.79,\n          0.582\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12168303467913538,\n        \"min\": 0.5314694491461194,\n        \"max\": 0.8328576513472774,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.5869045333635727,\n          0.5824766203235783,\n          0.7951148407378431\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1285169349311699,\n        \"min\": 0.505,\n        \"max\": 0.8299999999999998,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          0.8130000000000001,\n          0.79,\n          0.5820000000000001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Macro F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1314142171800935,\n        \"min\": 0.4897962957597078,\n        \"max\": 0.8305563610017777,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.5686293991865286,\n          0.5603709908303371,\n          0.7905800090300511\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E1Q4AmqJQjq6",
        "R1_896xfQtku",
        "ztI1l3YLmJ3a",
        "9HAZeE_ZCJeA",
        "f6cpBCyO-EHy",
        "m2QfRA_5_qNJ"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}